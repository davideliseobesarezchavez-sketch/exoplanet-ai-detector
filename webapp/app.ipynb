{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c0a90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. INSTALACI√ìN Y CONFIGURACI√ìN INICIAL\n",
    "# ======================================\n",
    "# Instalar bibliotecas necesarias\n",
    "!pip install streamlit joblib pandas numpy plotly scikit-learn lightgbm xgboost\n",
    "\n",
    "# Instalar ngrok y pyngrok para visualizar Streamlit en Colab\n",
    "!pip install pyngrok\n",
    "\n",
    "# Importar bibliotecas\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 2. GESTI√ìN DE RUTAS Y ESTRUCTURA DE CARPETAS PARA COLAB\n",
    "# ========================================================\n",
    "# Definir la ruta ra√≠z del proyecto para Colab\n",
    "# En Colab, el script se ejecuta en el directorio actual, as√≠ que lo usamos como ra√≠z.\n",
    "def get_project_root():\n",
    "    \"\"\"Obtener la ruta ra√≠z del proyecto: el directorio actual en Colab.\"\"\"\n",
    "    return os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "\n",
    "# Crear la estructura de directorios necesaria\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'data', 'raw'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'models'), exist_ok=True)\n",
    "\n",
    "# NOTA: Para que el entrenamiento funcione, debes subir tus archivos:\n",
    "# kepler.csv, k2.csv y tess.csv\n",
    "# a la carpeta 'data/raw' que acabamos de crear en el entorno de Colab.\n",
    "# Puedes hacerlo manualmente usando el explorador de archivos de Colab (icono de carpeta).\n",
    "print(f\"Carpeta de datos creada en: {os.path.join(PROJECT_ROOT, 'data', 'raw')}\")\n",
    "print(\"¬°Recuerda subir tus CSV de la NASA a esta carpeta!\")\n",
    "\n",
    "# 3. DEFINICI√ìN DE CLASES Y FUNCIONES (Tu c√≥digo original)\n",
    "# =========================================================\n",
    "\n",
    "class ExoplanetDataProcessor:\n",
    "    \"\"\"Procesador de datos para los datasets reales de la NASA\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "\n",
    "    def load_real_data(self):\n",
    "        \"\"\"Cargar los datasets reales de la NASA con rutas corregidas\"\"\"\n",
    "        try:\n",
    "            # Usar rutas absolutas desde la ra√≠z del proyecto\n",
    "            data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "\n",
    "            st.info(f\"üîç Buscando datos en: {data_dir}\")\n",
    "\n",
    "            # Listar archivos en el directorio\n",
    "            if os.path.exists(data_dir):\n",
    "                files = os.listdir(data_dir)\n",
    "                st.info(f\"üìÅ Archivos encontrados en data/raw/: {files}\")\n",
    "            else:\n",
    "                st.error(f\"‚ùå No existe el directorio: {data_dir}\")\n",
    "                return None, None, None\n",
    "\n",
    "            # Construir rutas completas\n",
    "            kepler_path = os.path.join(data_dir, 'kepler.csv')\n",
    "            k2_path = os.path.join(data_dir, 'k2.csv')\n",
    "            tess_path = os.path.join(data_dir, 'tess.csv')\n",
    "\n",
    "            st.info(f\"üìä Intentando cargar:\\n- {kepler_path}\\n- {k2_path}\\n- {tess_path}\")\n",
    "\n",
    "            # Verificar que los archivos existen\n",
    "            if not os.path.exists(kepler_path):\n",
    "                st.error(f\"‚ùå No existe: {kepler_path}\")\n",
    "                # Buscar archivos similares\n",
    "                csv_files = [f for f in files if f.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    st.info(f\"üìÑ Archivos CSV disponibles: {csv_files}\")\n",
    "                return None, None, None\n",
    "\n",
    "            # Cargar los archivos reales\n",
    "            kepler_df = pd.read_csv(kepler_path)\n",
    "            k2_df = pd.read_csv(k2_path) if os.path.exists(k2_path) else None\n",
    "            tess_df = pd.read_csv(tess_path) if os.path.exists(tess_path) else None\n",
    "\n",
    "            st.success(f\"‚úÖ Kepler cargado: {len(kepler_df)} registros\")\n",
    "            if k2_df is not None:\n",
    "                st.success(f\"‚úÖ K2 cargado: {len(k2_df)} registros\")\n",
    "            if tess_df is not None:\n",
    "                st.success(f\"‚úÖ TESS cargado: {len(tess_df)} registros\")\n",
    "\n",
    "            return kepler_df, k2_df, tess_df\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error cargando datasets: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def preprocess_kepler(self, df):\n",
    "        \"\"\"Preprocesar datos Kepler reales - VERSI√ìN MEJORADA\"\"\"\n",
    "        df_clean = df.copy()\n",
    "\n",
    "        st.info(\"üîß Procesando datos Kepler...\")\n",
    "\n",
    "        # Mostrar columnas disponibles\n",
    "        st.write(f\"üìã Columnas en Kepler: {list(df_clean.columns)}\")\n",
    "\n",
    "        # Verificar si existe la columna de target\n",
    "        if 'koi_disposition' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'koi_disposition' en Kepler\")\n",
    "            st.info(\"Las columnas disponibles son:\")\n",
    "            st.write(list(df_clean.columns))\n",
    "            return df_clean\n",
    "\n",
    "        # Eliminar columnas no √∫tiles (basado en el paper)\n",
    "        columns_to_drop = ['kepid', 'kepoi_name', 'kepler_name', 'koi_pdisposition', 'koi_score']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df_clean.columns]\n",
    "\n",
    "        if columns_to_drop:\n",
    "            df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "            st.write(f\"üóëÔ∏è Columnas eliminadas: {columns_to_drop}\")\n",
    "\n",
    "        # Mostrar valores √∫nicos en la columna de disposici√≥n\n",
    "        st.write(f\"üéØ Valores en koi_disposition: {df_clean['koi_disposition'].unique()}\")\n",
    "\n",
    "        # Filtrar solo confirmed, candidate y false positive\n",
    "        valid_dispositions = ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']\n",
    "        mask = df_clean['koi_disposition'].isin(valid_dispositions)\n",
    "        df_clean = df_clean[mask]\n",
    "\n",
    "        st.write(f\"üìä Distribuci√≥n despu√©s de filtrar: {df_clean['koi_disposition'].value_counts().to_dict()}\")\n",
    "\n",
    "        # Crear target binario\n",
    "        df_clean['target'] = df_clean['koi_disposition'].map({\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1,\n",
    "            'FALSE POSITIVE': 0\n",
    "        })\n",
    "\n",
    "        # A√±adir identificador de misi√≥n\n",
    "        df_clean['mission'] = 'kepler'\n",
    "\n",
    "        st.success(f\"‚úÖ Kepler procesado: {len(df_clean)} registros\")\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "    def preprocess_k2(self, df):\n",
    "        \"\"\"Preprocesar datos K2 reales\"\"\"\n",
    "        if df is None:\n",
    "            st.warning(\"‚ö†Ô∏è Dataset K2 no disponible\")\n",
    "            return None\n",
    "\n",
    "        df_clean = df.copy()\n",
    "\n",
    "        st.info(\"üîß Procesando datos K2...\")\n",
    "        st.write(f\"üìã Columnas en K2: {list(df_clean.columns)}\")\n",
    "\n",
    "        # Verificar columnas necesarias\n",
    "        if 'disposition' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'disposition' en K2\")\n",
    "            return None\n",
    "\n",
    "        # Filtrar solo confirmed y candidate\n",
    "        df_clean = df_clean[df_clean['disposition'].isin(['CONFIRMED', 'CANDIDATE'])]\n",
    "\n",
    "        # Target binario\n",
    "        df_clean['target'] = df_clean['disposition'].map({\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1\n",
    "        })\n",
    "\n",
    "        # Identificador de misi√≥n\n",
    "        df_clean['mission'] = 'k2'\n",
    "\n",
    "        st.success(f\"‚úÖ K2 procesado: {len(df_clean)} registros\")\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "    def preprocess_tess(self, df):\n",
    "        \"\"\"Preprocesar datos TESS reales\"\"\"\n",
    "        if df is None:\n",
    "            st.warning(\"‚ö†Ô∏è Dataset TESS no disponible\")\n",
    "            return None\n",
    "\n",
    "        df_clean = df.copy()\n",
    "\n",
    "        st.info(\"üîß Procesando datos TESS...\")\n",
    "        st.write(f\"üìã Columnas en TESS: {list(df_clean.columns)}\")\n",
    "\n",
    "        # Verificar columnas necesarias\n",
    "        if 'tfopwg_disp' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'tfopwg_disp' en TESS\")\n",
    "            return None\n",
    "\n",
    "        # Mapear disposiciones de TESS\n",
    "        disposition_mapping = {\n",
    "            'PC': 1, 'KP': 1, 'APC': 1,  # Positivos\n",
    "            'FP': 0, 'FA': 0  # Negativos\n",
    "        }\n",
    "\n",
    "        df_clean['target'] = df_clean['tfopwg_disp'].map(disposition_mapping)\n",
    "        df_clean = df_clean.dropna(subset=['target'])\n",
    "\n",
    "        # Identificador de misi√≥n\n",
    "        df_clean['mission'] = 'tess'\n",
    "\n",
    "        st.success(f\"‚úÖ TESS procesado: {len(df_clean)} registros\")\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Preparar caracter√≠sticas para el modelo - VERSI√ìN FLEXIBLE\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            st.error(\"‚ùå No hay datos para preparar caracter√≠sticas\")\n",
    "            return None, None, None\n",
    "\n",
    "        st.info(\"üîß Preparando caracter√≠sticas...\")\n",
    "\n",
    "        # Posibles nombres de columnas en diferentes datasets\n",
    "        possible_features = {\n",
    "            'orbital_period': ['koi_period', 'pl_orbper', 'period'],\n",
    "            'transit_duration': ['koi_duration', 'pl_trandurh', 'duration'],\n",
    "            'transit_depth': ['koi_depth', 'pl_trandep', 'depth'],\n",
    "            'planet_radius': ['koi_prad', 'pl_rade', 'radius'],\n",
    "            'equilibrium_temp': ['koi_teq', 'pl_eqt', 'teq'],\n",
    "            'insolation_flux': ['koi_insol', 'pl_insol', 'insol'],\n",
    "            'stellar_teff': ['koi_steff', 'st_teff', 'teff'],\n",
    "            'stellar_logg': ['koi_slogg', 'st_logg', 'logg'],\n",
    "            'stellar_radius': ['koi_srad', 'st_rad', 'srad']\n",
    "        }\n",
    "\n",
    "        # Encontrar las columnas disponibles\n",
    "        available_columns = []\n",
    "        for feature_name, possible_names in possible_features.items():\n",
    "            for name in possible_names:\n",
    "                if name in df.columns:\n",
    "                    available_columns.append(name)\n",
    "                    break\n",
    "\n",
    "        st.write(f\"üìä Columnas num√©ricas encontradas: {available_columns}\")\n",
    "\n",
    "        if not available_columns:\n",
    "            st.error(\"‚ùå No se encontraron columnas num√©ricas para entrenar\")\n",
    "            return None, None, None\n",
    "\n",
    "        X = df[available_columns].copy()\n",
    "        y = df['target'].values\n",
    "\n",
    "        st.write(f\"üìä Shape de X: {X.shape}, Shape de y: {y.shape}\")\n",
    "\n",
    "        # Manejar valores missing\n",
    "        missing_before = X.isnull().sum().sum()\n",
    "        X = X.fillna(X.median())\n",
    "        missing_after = X.isnull().sum().sum()\n",
    "\n",
    "        st.write(f\"üîß Valores missing: {missing_before} antes, {missing_after} despu√©s\")\n",
    "\n",
    "        # Escalar caracter√≠sticas\n",
    "        # La forma correcta de escalar para entrenar es:\n",
    "        # 1. Ajustar el escalador (fit_transform) si est√°s en la etapa de entrenamiento.\n",
    "        # 2. Transformar (transform) si est√°s en la etapa de predicci√≥n.\n",
    "        # Aqu√≠ es entrenamiento:\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.feature_names = available_columns\n",
    "\n",
    "        st.success(f\"‚úÖ Caracter√≠sticas preparadas: {X_scaled.shape}\")\n",
    "\n",
    "        return X_scaled, y, available_columns\n",
    "\n",
    "class RealExoplanetModel:\n",
    "    \"\"\"Modelo real para entrenamiento con datos de la NASA\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.accuracy = 0\n",
    "        self.feature_importance = None\n",
    "\n",
    "    def create_ensemble(self):\n",
    "        \"\"\"Crear ensemble con los algoritmos del paper\"\"\"\n",
    "        base_models = [\n",
    "            ('random_forest', RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                min_samples_split=5,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('extra_trees', ExtraTreesClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=10,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('xgboost', XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                random_state=42\n",
    "            )),\n",
    "            ('lightgbm', LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                random_state=42\n",
    "            ))\n",
    "        ]\n",
    "\n",
    "        ensemble = StackingClassifier(\n",
    "            estimators=base_models,\n",
    "            final_estimator=LogisticRegression(),\n",
    "            cv=3,  # Reducido para mayor velocidad\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        return ensemble\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Entrenar el modelo real\"\"\"\n",
    "        if X is None or y is None:\n",
    "            st.error(\"‚ùå No hay datos para entrenar\")\n",
    "            return None\n",
    "\n",
    "        st.info(\"ü§ñ Iniciando entrenamiento del ensemble...\")\n",
    "\n",
    "        self.model = self.create_ensemble()\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        # Calcular accuracy en entrenamiento\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "        st.write(f\"üìà Accuracy en entrenamiento: {self.accuracy:.2%}\")\n",
    "\n",
    "        # Calcular importancia de caracter√≠sticas\n",
    "        self._calculate_feature_importance(X.shape[1])\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def _calculate_feature_importance(self, n_features):\n",
    "        \"\"\"Calcular importancia de caracter√≠sticas promediada\"\"\"\n",
    "        importances = np.zeros(n_features)\n",
    "\n",
    "        for name, model in self.model.named_estimators_.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                # Los modelos de √°rbol (RF, ET, XGB, LGBM) tienen feature_importances_\n",
    "                importances += model.feature_importances_\n",
    "            # Nota: StackingClassifier y LogisticRegression (final_estimator) no tienen\n",
    "            # este atributo, por lo que solo promediamos sobre los que s√≠ lo tienen.\n",
    "\n",
    "        estimators_with_importance = sum(1 for name, model in self.model.named_estimators_.items() if hasattr(model, 'feature_importances_'))\n",
    "\n",
    "        if estimators_with_importance > 0:\n",
    "            self.feature_importance = importances / estimators_with_importance\n",
    "        else:\n",
    "            self.feature_importance = None # No se pudo calcular\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Guardar modelo entrenado\"\"\"\n",
    "        if self.model:\n",
    "            # Asegurar que el directorio existe\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            joblib.dump(self.model, filepath)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Cargar modelo entrenado\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(filepath):\n",
    "                self.model = joblib.load(filepath)\n",
    "                # Intenta cargar el accuracy si existe un archivo de m√©tricas,\n",
    "                # pero para simplificar lo dejamos en 0.\n",
    "                self.accuracy = 0\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error cargando modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "class ExoplanetDetectorApp:\n",
    "    def __init__(self):\n",
    "        self.model = RealExoplanetModel()\n",
    "        self.data_processor = ExoplanetDataProcessor()\n",
    "        self.model_trained = False\n",
    "\n",
    "        # Autocargar modelo y preprocesador al inicio si existen\n",
    "        self.load_initial_state()\n",
    "\n",
    "    def load_initial_state(self):\n",
    "        \"\"\"Cargar modelo y preprocesador guardados si existen\"\"\"\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            if self.model.load_model(model_path):\n",
    "                self.model_trained = True\n",
    "\n",
    "        if os.path.exists(processor_path):\n",
    "            try:\n",
    "                self.data_processor = joblib.load(processor_path)\n",
    "            except Exception as e:\n",
    "                # Si falla la carga del procesador, se ignora y se usa el nuevo.\n",
    "                pass\n",
    "\n",
    "        if os.path.exists(features_path):\n",
    "            try:\n",
    "                self.data_processor.feature_names = joblib.load(features_path)\n",
    "            except Exception as e:\n",
    "                # Si falla la carga de nombres, se ignora.\n",
    "                pass\n",
    "\n",
    "\n",
    "    def render_sidebar(self):\n",
    "        \"\"\"Barra lateral de navegaci√≥n - ACTUALIZADA\"\"\"\n",
    "        st.sidebar.title(\"üî≠ NASA Exoplanet Detector - REAL\")\n",
    "        st.sidebar.markdown(\"---\")\n",
    "\n",
    "        page = st.sidebar.radio(\"Navegaci√≥n\", [\n",
    "            \"üè† Inicio\",\n",
    "            \"üöÄ Entrenar Modelo REAL\",\n",
    "            \"ü§ñ Clasificar Exoplanetas\",\n",
    "            \"üì¶ Clasificaci√≥n por Lotes\",\n",
    "            \"üìä An√°lisis de Datos REAL\",\n",
    "            \"üíæ Modelos Guardados\"\n",
    "        ])\n",
    "\n",
    "        st.sidebar.markdown(\"---\")\n",
    "        st.sidebar.info(\n",
    "            \"Sistema REAL con datos de Kepler, K2 y TESS de la NASA\"\n",
    "        )\n",
    "\n",
    "        return page\n",
    "\n",
    "    def render_home(self):\n",
    "        \"\"\"P√°gina de inicio\"\"\"\n",
    "        st.title(\"ü™ê NASA Exoplanet Detection AI - SISTEMA REAL\")\n",
    "\n",
    "        col1, col2 = st.columns([2, 1])\n",
    "\n",
    "        with col1:\n",
    "            st.markdown(\"\"\"\n",
    "            ### Sistema REAL de Detecci√≥n de Exoplanetas\n",
    "\n",
    "            **Caracter√≠sticas IMPLEMENTADAS:**\n",
    "            - ‚úÖ **Entrenamiento REAL** con datos de la NASA\n",
    "            - ‚úÖ **Modelos PERSISTENTES** que se guardan en disco\n",
    "            - ‚úÖ **Datos REALES** Kepler, K2 y TESS\n",
    "            - ‚úÖ **Ensemble Stacking** como en el paper cient√≠fico\n",
    "            - ‚úÖ **Guardado/Auto-carga** de modelos\n",
    "            \"\"\")\n",
    "\n",
    "            st.warning(\"\"\"\n",
    "            **ANTES DE CONTINUAR EN COLAB:**\n",
    "            1.  **Sube tus CSV** (`kepler.csv`, `k2.csv`, `tess.csv`)\n",
    "                a la carpeta `data/raw/` en el explorador de archivos de Colab.\n",
    "            2.  Luego, ve a **'üöÄ Entrenar Modelo REAL'**\n",
    "            \"\"\")\n",
    "\n",
    "        with col2:\n",
    "            st.image(\"https://www.nasa.gov/sites/default/files/thumbnails/image/kepler_all_planets_art.jpg\",\n",
    "                     use_column_width=True,\n",
    "                     caption=\"Datos REALES de la NASA\")\n",
    "\n",
    "        # Verificar estructura de archivos\n",
    "        st.subheader(\"üîç Verificaci√≥n de Archivos\")\n",
    "\n",
    "        data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "        if os.path.exists(data_dir):\n",
    "            files = os.listdir(data_dir)\n",
    "            csv_files = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "            if csv_files:\n",
    "                st.success(f\"‚úÖ Directorio data/raw/ encontrado\")\n",
    "                st.write(f\"üìÑ Archivos CSV: {csv_files}\")\n",
    "            else:\n",
    "                st.warning(f\"‚ö†Ô∏è Directorio existe pero no hay archivos CSV\")\n",
    "        else:\n",
    "            st.error(f\"‚ùå No existe el directorio: {data_dir}\")\n",
    "\n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if os.path.exists(model_path):\n",
    "            st.success(\"‚úÖ **Modelo entrenado disponible** - Puedes usarlo en 'Clasificar Exoplanetas'\")\n",
    "            if self.model_trained: # Ya se carg√≥ en load_initial_state\n",
    "                st.metric(\"Modelo Cargado\", \"Ensemble Stacking\")\n",
    "            else:\n",
    "                st.warning(\"Modelo existe, pero la autocarga fall√≥. Vuelve a entrenar si es necesario.\")\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è **No hay modelo entrenado** - Ve a 'Entrenar Modelo REAL' para comenzar\")\n",
    "\n",
    "    def render_real_training(self):\n",
    "        \"\"\"P√°gina de entrenamiento REAL con datos de la NASA\"\"\"\n",
    "        st.title(\"üöÄ Entrenamiento REAL con Datos NASA\")\n",
    "\n",
    "        st.info(\"\"\"\n",
    "        **Entrenamiento REAL del modelo Ensemble** usando tus datasets de:\n",
    "        - Kepler.csv (datos reales)\n",
    "        - K2.csv (datos reales)\n",
    "        - TESS.csv (datos reales)\n",
    "\n",
    "        El modelo entrenado se guardar√° autom√°ticamente y estar√° disponible para clasificaci√≥n.\n",
    "        \"\"\")\n",
    "\n",
    "        if st.button(\"üéØ Iniciar Entrenamiento REAL\", type=\"primary\"):\n",
    "            with st.spinner(\"Cargando y procesando datos REALES de la NASA...\"):\n",
    "                try:\n",
    "                    # Cargar datos reales\n",
    "                    kepler_df, k2_df, tess_df = self.data_processor.load_real_data()\n",
    "\n",
    "                    if kepler_df is None or kepler_df.empty:\n",
    "                        st.error(\"\"\"\n",
    "                        ‚ùå **No se pudieron cargar los datasets o Kepler est√° vac√≠o**\n",
    "\n",
    "                        **Posibles soluciones:**\n",
    "                        1. Verifica que los archivos est√©n en `data/raw/`\n",
    "                        2. Aseg√∫rate de que se llamen `kepler.csv`, `k2.csv`, `tess.csv`\n",
    "                        3. Verifica que los archivos no est√©n corruptos\n",
    "                        \"\"\")\n",
    "                        return\n",
    "\n",
    "                    # Mostrar informaci√≥n de los datasets\n",
    "                    st.subheader(\"üìä Datasets Cargados\")\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "\n",
    "                    with col1:\n",
    "                        st.metric(\"Kepler\", f\"{len(kepler_df):,} registros\")\n",
    "                    with col2:\n",
    "                        k2_count = len(k2_df) if k2_df is not None else 0\n",
    "                        st.metric(\"K2\", f\"{k2_count:,} registros\")\n",
    "                    with col3:\n",
    "                        tess_count = len(tess_df) if tess_df is not None else 0\n",
    "                        st.metric(\"TESS\", f\"{tess_count:,} registros\")\n",
    "\n",
    "                    # Procesar datos\n",
    "                    st.subheader(\"üîß Procesando Datos...\")\n",
    "\n",
    "                    # Preprocesar Kepler\n",
    "                    kepler_processed = self.data_processor.preprocess_kepler(kepler_df)\n",
    "                    if kepler_processed is None:\n",
    "                        return\n",
    "\n",
    "                    # Preprocesar K2 y TESS si est√°n disponibles\n",
    "                    datasets_to_process = [kepler_processed]\n",
    "\n",
    "                    if k2_df is not None:\n",
    "                        k2_processed = self.data_processor.preprocess_k2(k2_df)\n",
    "                        if k2_processed is not None:\n",
    "                            datasets_to_process.append(k2_processed)\n",
    "\n",
    "                    if tess_df is not None:\n",
    "                        tess_processed = self.data_processor.preprocess_tess(tess_df)\n",
    "                        if tess_processed is not None:\n",
    "                            datasets_to_process.append(tess_processed)\n",
    "\n",
    "                    # Unificar datos (filtrar None values)\n",
    "                    datasets_to_process = [d for d in datasets_to_process if d is not None and not d.empty]\n",
    "                    if not datasets_to_process:\n",
    "                        st.error(\"‚ùå No hay datos v√°lidos para procesar\")\n",
    "                        return\n",
    "\n",
    "                    unified_data = pd.concat(datasets_to_process, ignore_index=True)\n",
    "\n",
    "                    st.success(f\"‚úÖ Datos unificados: {len(unified_data):,} muestras\")\n",
    "\n",
    "                    # Preparar caracter√≠sticas\n",
    "                    X, y, feature_names = self.data_processor.prepare_features(unified_data)\n",
    "\n",
    "                    if X is None:\n",
    "                        st.error(\"‚ùå No se pudieron preparar las caracter√≠sticas\")\n",
    "                        return\n",
    "\n",
    "                    # Entrenar modelo\n",
    "                    st.subheader(\"ü§ñ Entrenando Modelo Ensemble...\")\n",
    "                    trained_model = self.model.train(X, y)\n",
    "\n",
    "                    if trained_model is None:\n",
    "                        st.error(\"‚ùå Error en el entrenamiento\")\n",
    "                        return\n",
    "\n",
    "                    # Guardar modelo\n",
    "                    models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "                    model_path = os.path.join(models_dir, 'real_ensemble_model.pkl')\n",
    "                    processor_path = os.path.join(models_dir, 'data_processor.pkl')\n",
    "                    features_path = os.path.join(models_dir, 'feature_names.pkl')\n",
    "\n",
    "                    model_saved = self.model.save_model(model_path)\n",
    "\n",
    "                    if model_saved:\n",
    "                        # Guardar tambi√©n el preprocesador y feature names\n",
    "                        joblib.dump(self.data_processor, processor_path)\n",
    "                        joblib.dump(feature_names, features_path)\n",
    "\n",
    "                        st.success(\"‚úÖ Modelo entrenado y guardado exitosamente!\")\n",
    "                        self.model_trained = True\n",
    "\n",
    "                        # Mostrar resultados\n",
    "                        st.subheader(\"üìà Resultados del Entrenamiento\")\n",
    "                        col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "                        with col1:\n",
    "                            st.metric(\"Accuracy\", f\"{self.model.accuracy:.2%}\")\n",
    "                        with col2:\n",
    "                            st.metric(\"Muestras\", f\"{X.shape[0]:,}\")\n",
    "                        with col3:\n",
    "                            st.metric(\"Caracter√≠sticas\", X.shape[1])\n",
    "                        with col4:\n",
    "                            st.metric(\"Algoritmos\", \"4 Ensemble\")\n",
    "\n",
    "                        # Importancia de caracter√≠sticas\n",
    "                        if self.model.feature_importance is not None and feature_names:\n",
    "                            st.subheader(\"üîç Importancia de Caracter√≠sticas\")\n",
    "                            # Asegurarse de que las longitudes coincidan antes de crear el DataFrame\n",
    "                            if len(feature_names) == len(self.model.feature_importance):\n",
    "                                importance_df = pd.DataFrame({\n",
    "                                    'Caracter√≠stica': feature_names,\n",
    "                                    'Importancia': self.model.feature_importance\n",
    "                                }).sort_values('Importancia', ascending=False)\n",
    "\n",
    "                                fig = px.bar(\n",
    "                                    importance_df.head(10),\n",
    "                                    x='Importancia',\n",
    "                                    y='Caracter√≠stica',\n",
    "                                    title='Top 10 Caracter√≠sticas M√°s Importantes',\n",
    "                                    orientation='h'\n",
    "                                )\n",
    "                                st.plotly_chart(fig, use_container_width=True)\n",
    "                            else:\n",
    "                                st.warning(\"No se pudo graficar la importancia: longitudes de nombres y valores no coinciden.\")\n",
    "\n",
    "\n",
    "                    st.balloons()\n",
    "\n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error durante el entrenamiento: {str(e)}\")\n",
    "                    import traceback\n",
    "                    st.code(traceback.format_exc())\n",
    "\n",
    "    def render_real_classification(self):\n",
    "        \"\"\"Clasificaci√≥n con modelo REAL entrenado - VERSI√ìN CORREGIDA\"\"\"\n",
    "        st.title(\"ü§ñ Clasificaci√≥n con Modelo REAL\")\n",
    "\n",
    "        # Rutas de archivos\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "\n",
    "        # Verificar si hay modelo entrenado\n",
    "        if not os.path.exists(model_path) or not os.path.exists(features_path) or not os.path.exists(processor_path):\n",
    "            st.warning(\"\"\"\n",
    "            ‚ö†Ô∏è **No hay modelo o archivos de preprocesamiento entrenados**\n",
    "\n",
    "            Para usar el clasificador REAL:\n",
    "            1. Ve a la pesta√±a **'Entrenar Modelo REAL'**\n",
    "            2. Entrena el modelo con tus datos de la NASA\n",
    "            3. Regresa aqu√≠ para clasificar candidatos\n",
    "            \"\"\")\n",
    "            return\n",
    "\n",
    "        # Cargar modelo y preprocesador si a√∫n no se han cargado\n",
    "        if not self.model_trained:\n",
    "            self.load_initial_state()\n",
    "            if not self.model_trained:\n",
    "                st.error(\"‚ùå Error cargando el modelo o el preprocesador.\")\n",
    "                return\n",
    "\n",
    "        st.success(\"‚úÖ Modelo REAL y preprocesador cargados exitosamente\")\n",
    "\n",
    "        # Cargar feature names para mostrar en la interfaz\n",
    "        feature_names = self.data_processor.feature_names\n",
    "        num_features = len(feature_names)\n",
    "        st.info(f\"üîç El modelo espera **{num_features}** caracter√≠sticas: {', '.join(feature_names)}\")\n",
    "\n",
    "        st.info(\"\"\"\n",
    "        üîç **Clasificador REAL**: Introduce los par√°metros astron√≥micos que el modelo espera.\n",
    "        \"\"\")\n",
    "\n",
    "        # Definir un diccionario para los inputs\n",
    "        input_values = {}\n",
    "        # Definir los nombres amigables y valores por defecto (basado en el c√≥digo original)\n",
    "        input_definitions = [\n",
    "            (\"koi_period\", \"Per√≠odo Orbital (d√≠as)\", 10.0, 0.1, 1000.0),\n",
    "            (\"koi_duration\", \"Duraci√≥n Tr√°nsito (horas)\", 3.0, 0.1, 24.0),\n",
    "            (\"koi_depth\", \"Profundidad Tr√°nsito (ppm)\", 500, 1, 100000),\n",
    "            (\"koi_prad\", \"Radio Planetario (Radios Tierra)\", 2.0, 0.1, 50.0),\n",
    "            (\"koi_teq\", \"Temperatura Equilibrio (K)\", 500, 100, 5000),\n",
    "            (\"koi_insol\", \"Flujo de Insolaci√≥n\", 100.0, 0.1, 10000.0),\n",
    "            (\"koi_steff\", \"Temperatura Estelar (K)\", 5800, 2000, 15000),\n",
    "            (\"koi_slogg\", \"Gravedad Estelar (log g)\", 4.4, 3.0, 5.5),\n",
    "            (\"koi_srad\", \"Radio Estelar (Radios Sol)\", 1.0, 0.1, 10.0),\n",
    "        ]\n",
    "\n",
    "        # Mapear los nombres de Kepler a sus definiciones\n",
    "        feature_map = {name: defs for name, *defs in input_definitions}\n",
    "\n",
    "        with st.form(\"real_classification_form\"):\n",
    "            st.subheader(f\"üìê Par√°metros del Candidato - {num_features} CARACTER√çSTICAS REQUERIDAS\")\n",
    "\n",
    "            # Crear columnas din√°micas (m√°ximo 2 columnas)\n",
    "            cols = st.columns(min(2, num_features))\n",
    "            col_index = 0\n",
    "\n",
    "            # Solo mostrar los inputs para las caracter√≠sticas que el modelo REAL espera\n",
    "            for feature_name in feature_names:\n",
    "                if feature_name in feature_map:\n",
    "                    friendly_name, default_value, min_val, max_val = feature_map[feature_name]\n",
    "                    with cols[col_index % 2]:\n",
    "                        input_values[feature_name] = st.number_input(\n",
    "                            friendly_name,\n",
    "                            min_value=min_val,\n",
    "                            max_value=max_val,\n",
    "                            value=default_value,\n",
    "                            key=f\"input_{feature_name}\"\n",
    "                        )\n",
    "                    col_index += 1\n",
    "                else:\n",
    "                    st.warning(f\"‚ö†Ô∏è Caracter√≠stica esperada **{feature_name}** no encontrada en la definici√≥n del formulario.\")\n",
    "\n",
    "\n",
    "            submitted = st.form_submit_button(\"üöÄ Clasificar con Modelo REAL\")\n",
    "\n",
    "        if submitted:\n",
    "            # Ordenar las caracter√≠sticas de entrada seg√∫n el orden esperado por el modelo entrenado\n",
    "            final_features = tuple(input_values[name] for name in feature_names if name in input_values)\n",
    "\n",
    "            if len(final_features) == num_features:\n",
    "                st.info(f\"üîç Enviando {len(final_features)} caracter√≠sticas al modelo: {', '.join(feature_names)}\")\n",
    "                self._real_prediction(*final_features)\n",
    "            else:\n",
    "                 st.error(f\"\"\"\n",
    "                ‚ùå **ERROR DE CARACTER√çSTICAS**\n",
    "\n",
    "                **Env√≠as:** {len(final_features)} caracter√≠sticas\n",
    "                **Modelo espera:** {num_features} caracter√≠sticas ({', '.join(feature_names)})\n",
    "\n",
    "                **Soluci√≥n:** Aseg√∫rate de que todos los campos del formulario coincidan\n",
    "                con las caracter√≠sticas esperadas por el modelo entrenado.\n",
    "                \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    def _real_prediction(self, *features):\n",
    "        \"\"\"Predicci√≥n REAL con el modelo entrenado - VERSI√ìN CORREGIDA\"\"\"\n",
    "        # Cargar informaci√≥n del modelo\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "\n",
    "        try:\n",
    "            # Recargar feature names y preprocesador para asegurar\n",
    "            saved_feature_names = joblib.load(features_path)\n",
    "            data_processor = joblib.load(processor_path)\n",
    "\n",
    "            # VERIFICACI√ìN CR√çTICA: ¬øCoincide el n√∫mero de caracter√≠sticas?\n",
    "            if len(features) != len(saved_feature_names):\n",
    "                st.error(f\"\"\"\n",
    "                ‚ùå **ERROR CR√çTICO - Discrepancia en caracter√≠sticas**\n",
    "                **Env√≠as:** {len(features)} caracter√≠sticas\n",
    "                **Modelo espera:** {len(saved_feature_names)} caracter√≠sticas\n",
    "                \"\"\")\n",
    "                return\n",
    "\n",
    "            # Crear array de caracter√≠sticas\n",
    "            feature_array = np.array([features]).reshape(1, -1)\n",
    "\n",
    "            # Escalar caracter√≠sticas (usando el escalador entrenado)\n",
    "            feature_array_scaled = data_processor.scaler.transform(feature_array)\n",
    "\n",
    "            # Realizar predicci√≥n\n",
    "            prediction = self.model.model.predict(feature_array_scaled)[0]\n",
    "            probability = self.model.model.predict_proba(feature_array_scaled)[0, 1]\n",
    "\n",
    "            # Mostrar resultados\n",
    "            st.subheader(\"üéØ Resultado de la Clasificaci√≥n REAL\")\n",
    "\n",
    "            col1, col2 = st.columns([1, 2])\n",
    "\n",
    "            with col1:\n",
    "                if prediction == 1:\n",
    "                    st.success(\"‚úÖ **EXOPLANETA DETECTADO**\")\n",
    "                    st.balloons()\n",
    "                else:\n",
    "                    st.error(\"‚ùå **NO ES EXOPLANETA**\")\n",
    "\n",
    "                st.metric(\"Probabilidad\", f\"{probability:.2%}\")\n",
    "\n",
    "                # Interpretaci√≥n de la probabilidad\n",
    "                if probability >= 0.8:\n",
    "                    st.info(\"üü¢ **Alta confianza** - Muy probable exoplaneta\")\n",
    "                elif probability >= 0.6:\n",
    "                    st.info(\"üü° **Confianza media** - Posible exoplaneta\")\n",
    "                else:\n",
    "                    st.info(\"üî¥ **Baja confianza** - Probable falso positivo\")\n",
    "\n",
    "            with col2:\n",
    "                # An√°lisis detallado de caracter√≠sticas\n",
    "                st.markdown(\"#### üìä An√°lisis de Caracter√≠sticas\")\n",
    "\n",
    "                # Mapeo de nombres amigables\n",
    "                feature_display_names = {\n",
    "                    'koi_period': 'Per√≠odo Orbital',\n",
    "                    'koi_duration': 'Duraci√≥n Tr√°nsito',\n",
    "                    'koi_depth': 'Profundidad Tr√°nsito',\n",
    "                    'koi_prad': 'Radio Planetario',\n",
    "                    'koi_teq': 'Temperatura Planeta',\n",
    "                    'koi_insol': 'Flujo Insolaci√≥n',\n",
    "                    'koi_steff': 'Temperatura Estelar',\n",
    "                    'koi_slogg': 'Gravedad Estelar',\n",
    "                    'koi_srad': 'Radio Estelar'\n",
    "                }\n",
    "\n",
    "                # Mapeo de unidades\n",
    "                feature_units = {\n",
    "                    'koi_period': 'd√≠as',\n",
    "                    'koi_duration': 'horas',\n",
    "                    'koi_depth': 'ppm',\n",
    "                    'koi_prad': 'R‚äï', # Radios Tierra\n",
    "                    'koi_teq': 'K',\n",
    "                    'koi_insol': 'S‚äï', # Flujo Solar\n",
    "                    'koi_steff': 'K',\n",
    "                    'koi_slogg': 'log g',\n",
    "                    'koi_srad': 'R‚òâ' # Radios Sol\n",
    "                }\n",
    "\n",
    "                # Crear tabla de an√°lisis\n",
    "                analysis_data = []\n",
    "                for i, feature_name in enumerate(saved_feature_names):\n",
    "                    display_name = feature_display_names.get(feature_name, feature_name)\n",
    "                    units = feature_units.get(feature_name, '')\n",
    "                    value = features[i]\n",
    "\n",
    "                    analysis_data.append({\n",
    "                        'Caracter√≠stica': display_name,\n",
    "                        'Valor': f\"{value} {units}\",\n",
    "                        'C√≥digo': feature_name\n",
    "                    })\n",
    "\n",
    "                analysis_df = pd.DataFrame(analysis_data)\n",
    "                st.dataframe(analysis_df, use_container_width=True, hide_index=True)\n",
    "\n",
    "                # Informaci√≥n adicional\n",
    "                st.markdown(\"#### üí° Informaci√≥n del Modelo\")\n",
    "                st.info(f\"\"\"\n",
    "                - **Modelo:** Ensemble Stacking (4 algoritmos)\n",
    "                - **Caracter√≠sticas:** {len(saved_feature_names)}\n",
    "                - **Precisi√≥n (Entrenamiento):** ~{self.model.accuracy:.2%}\n",
    "                - **Datos de entrenamiento:** Kepler + K2 + TESS (NASA)\n",
    "                \"\"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error en la predicci√≥n: {e}\")\n",
    "            import traceback\n",
    "            st.code(traceback.format_exc())\n",
    "            st.info(\"üí° **Soluci√≥n:** Reentrena el modelo en la pesta√±a 'Entrenar Modelo REAL'\")\n",
    "\n",
    "    # Funciones de las pesta√±as faltantes para que no d√© error.\n",
    "    def render_batch_classification(self):\n",
    "        st.title(\"üì¶ Clasificaci√≥n por Lotes - (En Desarrollo)\")\n",
    "        st.info(\"Esta funcionalidad te permitir√° subir un archivo CSV para clasificar m√∫ltiples candidatos a exoplanetas.\")\n",
    "        st.warning(\"Deber√≠as implementar aqu√≠ la l√≥gica para subir un archivo, preprocesarlo (usando el DataProcessor cargado) y mostrar los resultados.\")\n",
    "\n",
    "    def render_data_analysis(self):\n",
    "        st.title(\"üìä An√°lisis de Datos REAL - (En Desarrollo)\")\n",
    "        st.info(\"Esta secci√≥n podr√≠a mostrar gr√°ficos de dispersi√≥n, histogramas de las caracter√≠sticas, y la distribuci√≥n de clases (CONFIRMED vs. FALSE POSITIVE) del dataset unificado.\")\n",
    "\n",
    "    def render_saved_models(self):\n",
    "        st.title(\"üíæ Modelos Guardados\")\n",
    "        models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "        st.info(f\"Buscando archivos en: {models_dir}\")\n",
    "        if os.path.exists(models_dir):\n",
    "            files = os.listdir(models_dir)\n",
    "            model_files = [f for f in files if f.endswith('.pkl')]\n",
    "            if model_files:\n",
    "                st.success(\"Archivos de modelo encontrados:\")\n",
    "                st.write(model_files)\n",
    "            else:\n",
    "                st.warning(\"No se encontraron archivos de modelo (.pkl).\")\n",
    "        else:\n",
    "            st.error(\"Directorio de modelos no encontrado.\")\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Funci√≥n principal para correr la aplicaci√≥n\"\"\"\n",
    "        st.set_page_config(layout=\"wide\", page_title=\"NASA Exoplanet Detector\")\n",
    "        page = self.render_sidebar()\n",
    "\n",
    "        if page == \"üè† Inicio\":\n",
    "            self.render_home()\n",
    "        elif page == \"üöÄ Entrenar Modelo REAL\":\n",
    "            self.render_real_training()\n",
    "        elif page == \"ü§ñ Clasificar Exoplanetas\":\n",
    "            self.render_real_classification()\n",
    "        elif page == \"üì¶ Clasificaci√≥n por Lotes\":\n",
    "            self.render_batch_classification()\n",
    "        elif page == \"üìä An√°lisis de Datos REAL\":\n",
    "            self.render_data_analysis()\n",
    "        elif page == \"üíæ Modelos Guardados\":\n",
    "            self.render_saved_models()\n",
    "\n",
    "# 4. FUNCI√ìN DE EJECUCI√ìN PRINCIPAL PARA COLAB\n",
    "# ============================================\n",
    "\n",
    "# Escribir el c√≥digo Streamlit en un archivo temporal (app.py) para que ngrok lo ejecute.\n",
    "# Necesitamos que todo el c√≥digo est√© dentro de este archivo para la ejecuci√≥n de Streamlit.\n",
    "\n",
    "# La forma m√°s simple en Colab es encapsular la funci√≥n de arranque en un bloque __name__ == '__main__'\n",
    "# y luego usar un servidor local para abrir Streamlit.\n",
    "\n",
    "# Escribir el contenido principal de la app en un archivo app.py\n",
    "APP_CODE = \"\"\"\n",
    "# webapp/app.py - Contenido para ejecuci√≥n de Streamlit\n",
    "# Este archivo encapsula tu c√≥digo para ser ejecutado por streamlit run\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# OBTENER LA RUTA CORRECTA DEL PROYECTO\n",
    "def get_project_root():\n",
    "    # En Colab, el directorio de ejecuci√≥n ser√° la ra√≠z\n",
    "    return os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "\n",
    "# =========================================================================\n",
    "# CLASES COPIADAS DEL BLOQUE DE C√ìDIGO ANTERIOR EN COLAB\n",
    "# Se asume que las clases ExoplanetDataProcessor, RealExoplanetModel y ExoplanetDetectorApp\n",
    "# est√°n definidas en el entorno de Colab ANTES de que se ejecute este bloque,\n",
    "# pero para la ejecuci√≥n correcta de 'streamlit run', DEBEN estar dentro del archivo.\n",
    "# Por simplicidad en la estructura de Colab, las pegamos completas en el string.\n",
    "# =========================================================================\n",
    "\n",
    "# NOTA: Por limitaciones de espacio/formato, aqu√≠ se omite la repetici√≥n de\n",
    "# TODAS las clases (ExoplanetDataProcessor, RealExoplanetModel, ExoplanetDetectorApp)\n",
    "# dentro de este string APP_CODE.\n",
    "# En un entorno real, copiar√≠as las definiciones completas de las clases aqu√≠.\n",
    "# Para el prop√≥sito de esta respuesta, el usuario debe copiar las clases\n",
    "# completas justo despu√©s de las importaciones dentro del archivo real 'app.py'\n",
    "# que se ejecuta en Colab.\n",
    "\n",
    "# Para la ejecuci√≥n en Colab, las clases ya est√°n en memoria,\n",
    "# pero 'streamlit run' necesita el archivo.\n",
    "# La soluci√≥n es usar una funci√≥n principal que llame a las clases\n",
    "# que ya est√°n definidas en el entorno global de Colab.\n",
    "\n",
    "# Re-definici√≥n de las clases (SE REQUIERE LA DEFINICI√ìN COMPLETA AQU√ç PARA 'streamlit run')\n",
    "# Para la prueba, usaremos la instancia global\n",
    "# =======================================================\n",
    "# Simulaci√≥n de la aplicaci√≥n real para el archivo app.py\n",
    "# (Requerir√≠a la definici√≥n completa de las clases aqu√≠)\n",
    "# =======================================================\n",
    "\n",
    "# Para el prop√≥sito de Colab, llamaremos directamente a las clases definidas en la celda anterior.\n",
    "\n",
    "def main():\n",
    "    # Re-definir las clases aqu√≠ COMPLETAMENTE (omitiendo por brevedad de la respuesta)\n",
    "    # Copia TODAS las clases y funciones (ExoplanetDataProcessor, RealExoplanetModel, ExoplanetDetectorApp, get_project_root)\n",
    "    # y p√©galas dentro de este string de Python\n",
    "    # ... CLASES COMPLETAS ...\n",
    "    \n",
    "    # Dado que estamos en Colab, es m√°s f√°cil usar un archivo temporal\n",
    "    # que simplemente llame a la funci√≥n run() de la clase.\n",
    "\n",
    "    # Para que funcione correctamente en Colab, debemos definir el c√≥digo\n",
    "    # del script que Streamlit va a ejecutar.\n",
    "    # Dado que el prompt contiene la versi√≥n completa y corregida, la usaremos.\n",
    "    # El string de c√≥digo final deber√≠a contener todo lo que se pas√≥ en el prompt.\n",
    "    \n",
    "    # Para evitar la complejidad de re-escribir el script,\n",
    "    # simplemente asume que el usuario copiar√° y pegar√° la soluci√≥n completa\n",
    "    # en una celda y usaremos una funci√≥n simple para ejecutar Streamlit.\n",
    "    \n",
    "    # Como alternativa, ejecuta la aplicaci√≥n directamente en la celda final:\n",
    "    # app = ExoplanetDetectorApp()\n",
    "    # app.run()\n",
    "\n",
    "    # Pero Streamlit requiere 'streamlit run file.py'\n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "# Pegar la definici√≥n completa de las clases en un archivo llamado 'app.py'\n",
    "# Este es el archivo que Streamlit ejecutar√°\n",
    "APP_FILE_CONTENT = f\"\"\"\n",
    "# webapp/app.py - VERSI√ìN CON RUTAS CORREGIDAS PARA COLAB\n",
    "{APP_CODE}\n",
    "\n",
    "# OBTENER LA RUTA CORRECTA DEL PROYECTO (Redefinici√≥n para el script)\n",
    "def get_project_root():\n",
    "    return os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "\n",
    "# =======================================================\n",
    "# CLASES Y FUNCIONES COMPLETAS DEL PROMPT\n",
    "# =======================================================\n",
    "\n",
    "{ExoplanetDataProcessor.__doc__}\n",
    "class ExoplanetDataProcessor:\n",
    "{ExoplanetDataProcessor.__init__.__doc__}\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "\n",
    "{ExoplanetDataProcessor.load_real_data.__doc__}\n",
    "    def load_real_data(self):\n",
    "        try:\n",
    "            data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "            st.info(f\"üîç Buscando datos en: {{data_dir}}\")\n",
    "            if os.path.exists(data_dir):\n",
    "                files = os.listdir(data_dir)\n",
    "                st.info(f\"üìÅ Archivos encontrados en data/raw/: {{files}}\")\n",
    "            else:\n",
    "                st.error(f\"‚ùå No existe el directorio: {{data_dir}}\")\n",
    "                return None, None, None\n",
    "\n",
    "            kepler_path = os.path.join(data_dir, 'kepler.csv')\n",
    "            k2_path = os.path.join(data_dir, 'k2.csv')\n",
    "            tess_path = os.path.join(data_dir, 'tess.csv')\n",
    "\n",
    "            st.info(f\"üìä Intentando cargar:\\\\n- {{kepler_path}}\\\\n- {{k2_path}}\\\\n- {{tess_path}}\")\n",
    "\n",
    "            if not os.path.exists(kepler_path):\n",
    "                st.error(f\"‚ùå No existe: {{kepler_path}}\")\n",
    "                csv_files = [f for f in files if f.endswith('.csv')]\n",
    "                if csv_files:\n",
    "                    st.info(f\"üìÑ Archivos CSV disponibles: {{csv_files}}\")\n",
    "                return None, None, None\n",
    "\n",
    "            kepler_df = pd.read_csv(kepler_path)\n",
    "            k2_df = pd.read_csv(k2_path) if os.path.exists(k2_path) else None\n",
    "            tess_df = pd.read_csv(tess_path) if os.path.exists(tess_path) else None\n",
    "\n",
    "            st.success(f\"‚úÖ Kepler cargado: {{len(kepler_df)}} registros\")\n",
    "            if k2_df is not None:\n",
    "                st.success(f\"‚úÖ K2 cargado: {{len(k2_df)}} registros\")\n",
    "            if tess_df is not None:\n",
    "                st.success(f\"‚úÖ TESS cargado: {{len(tess_df)}} registros\")\n",
    "\n",
    "            return kepler_df, k2_df, tess_df\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error cargando datasets: {{e}}\")\n",
    "            return None, None, None\n",
    "\n",
    "{ExoplanetDataProcessor.preprocess_kepler.__doc__}\n",
    "    def preprocess_kepler(self, df):\n",
    "        df_clean = df.copy()\n",
    "        st.info(\"üîß Procesando datos Kepler...\")\n",
    "        st.write(f\"üìã Columnas en Kepler: {{list(df_clean.columns)}}\")\n",
    "\n",
    "        if 'koi_disposition' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'koi_disposition' en Kepler\")\n",
    "            st.info(\"Las columnas disponibles son:\")\n",
    "            st.write(list(df_clean.columns))\n",
    "            return df_clean\n",
    "\n",
    "        columns_to_drop = ['kepid', 'kepoi_name', 'kepler_name', 'koi_pdisposition', 'koi_score']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df_clean.columns]\n",
    "\n",
    "        if columns_to_drop:\n",
    "            df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "            st.write(f\"üóëÔ∏è Columnas eliminadas: {{columns_to_drop}}\")\n",
    "\n",
    "        st.write(f\"üéØ Valores en koi_disposition: {{df_clean['koi_disposition'].unique()}}\")\n",
    "\n",
    "        valid_dispositions = ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']\n",
    "        mask = df_clean['koi_disposition'].isin(valid_dispositions)\n",
    "        df_clean = df_clean[mask]\n",
    "\n",
    "        st.write(f\"üìä Distribuci√≥n despu√©s de filtrar: {{df_clean['koi_disposition'].value_counts().to_dict()}}\")\n",
    "\n",
    "        df_clean['target'] = df_clean['koi_disposition'].map({{\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1,\n",
    "            'FALSE POSITIVE': 0\n",
    "        }})\n",
    "\n",
    "        df_clean['mission'] = 'kepler'\n",
    "        st.success(f\"‚úÖ Kepler procesado: {{len(df_clean)}} registros\")\n",
    "        return df_clean\n",
    "\n",
    "{ExoplanetDataProcessor.preprocess_k2.__doc__}\n",
    "    def preprocess_k2(self, df):\n",
    "        if df is None:\n",
    "            st.warning(\"‚ö†Ô∏è Dataset K2 no disponible\")\n",
    "            return None\n",
    "\n",
    "        df_clean = df.copy()\n",
    "        st.info(\"üîß Procesando datos K2...\")\n",
    "        st.write(f\"üìã Columnas en K2: {{list(df_clean.columns)}}\")\n",
    "\n",
    "        if 'disposition' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'disposition' en K2\")\n",
    "            return None\n",
    "\n",
    "        df_clean = df_clean[df_clean['disposition'].isin(['CONFIRMED', 'CANDIDATE'])]\n",
    "\n",
    "        df_clean['target'] = df_clean['disposition'].map({{\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1\n",
    "        }})\n",
    "\n",
    "        df_clean['mission'] = 'k2'\n",
    "        st.success(f\"‚úÖ K2 procesado: {{len(df_clean)}} registros\")\n",
    "        return df_clean\n",
    "\n",
    "{ExoplanetDataProcessor.preprocess_tess.__doc__}\n",
    "    def preprocess_tess(self, df):\n",
    "        if df is None:\n",
    "            st.warning(\"‚ö†Ô∏è Dataset TESS no disponible\")\n",
    "            return None\n",
    "\n",
    "        df_clean = df.copy()\n",
    "        st.info(\"üîß Procesando datos TESS...\")\n",
    "        st.write(f\"üìã Columnas en TESS: {{list(df_clean.columns)}}\")\n",
    "\n",
    "        if 'tfopwg_disp' not in df_clean.columns:\n",
    "            st.error(\"‚ùå No se encuentra la columna 'tfopwg_disp' en TESS\")\n",
    "            return None\n",
    "\n",
    "        disposition_mapping = {{\n",
    "            'PC': 1, 'KP': 1, 'APC': 1,\n",
    "            'FP': 0, 'FA': 0\n",
    "        }}\n",
    "\n",
    "        df_clean['target'] = df_clean['tfopwg_disp'].map(disposition_mapping)\n",
    "        df_clean = df_clean.dropna(subset=['target'])\n",
    "        df_clean['mission'] = 'tess'\n",
    "        st.success(f\"‚úÖ TESS procesado: {{len(df_clean)}} registros\")\n",
    "        return df_clean\n",
    "\n",
    "{ExoplanetDataProcessor.prepare_features.__doc__}\n",
    "    def prepare_features(self, df):\n",
    "        if df is None or len(df) == 0:\n",
    "            st.error(\"‚ùå No hay datos para preparar caracter√≠sticas\")\n",
    "            return None, None, None\n",
    "\n",
    "        st.info(\"üîß Preparando caracter√≠sticas...\")\n",
    "\n",
    "        possible_features = {{\n",
    "            'orbital_period': ['koi_period', 'pl_orbper', 'period'],\n",
    "            'transit_duration': ['koi_duration', 'pl_trandurh', 'duration'],\n",
    "            'transit_depth': ['koi_depth', 'pl_trandep', 'depth'],\n",
    "            'planet_radius': ['koi_prad', 'pl_rade', 'radius'],\n",
    "            'equilibrium_temp': ['koi_teq', 'pl_eqt', 'teq'],\n",
    "            'insolation_flux': ['koi_insol', 'pl_insol', 'insol'],\n",
    "            'stellar_teff': ['koi_steff', 'st_teff', 'teff'],\n",
    "            'stellar_logg': ['koi_slogg', 'st_logg', 'logg'],\n",
    "            'stellar_radius': ['koi_srad', 'st_rad', 'srad']\n",
    "        }}\n",
    "\n",
    "        available_columns = []\n",
    "        for feature_name, possible_names in possible_features.items():\n",
    "            for name in possible_names:\n",
    "                if name in df.columns:\n",
    "                    available_columns.append(name)\n",
    "                    break\n",
    "\n",
    "        st.write(f\"üìä Columnas num√©ricas encontradas: {{available_columns}}\")\n",
    "\n",
    "        if not available_columns:\n",
    "            st.error(\"‚ùå No se encontraron columnas num√©ricas para entrenar\")\n",
    "            return None, None, None\n",
    "\n",
    "        X = df[available_columns].copy()\n",
    "        y = df['target'].values\n",
    "        st.write(f\"üìä Shape de X: {{X.shape}}, Shape de y: {{y.shape}}\")\n",
    "\n",
    "        missing_before = X.isnull().sum().sum()\n",
    "        X = X.fillna(X.median())\n",
    "        missing_after = X.isnull().sum().sum()\n",
    "        st.write(f\"üîß Valores missing: {{missing_before}} antes, {{missing_after}} despu√©s\")\n",
    "\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.feature_names = available_columns\n",
    "        st.success(f\"‚úÖ Caracter√≠sticas preparadas: {{X_scaled.shape}}\")\n",
    "        return X_scaled, y, available_columns\n",
    "\n",
    "{RealExoplanetModel.__doc__}\n",
    "class RealExoplanetModel:\n",
    "{RealExoplanetModel.__init__.__doc__}\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.accuracy = 0\n",
    "        self.feature_importance = None\n",
    "\n",
    "{RealExoplanetModel.create_ensemble.__doc__}\n",
    "    def create_ensemble(self):\n",
    "        base_models = [\n",
    "            ('random_forest', RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42, n_jobs=-1)),\n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "            ('xgboost', XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)),\n",
    "            ('lightgbm', LGBMClassifier(n_estimators=100, learning_rate=0.05, max_depth=6, random_state=42))\n",
    "        ]\n",
    "\n",
    "        ensemble = StackingClassifier(\n",
    "            estimators=base_models,\n",
    "            final_estimator=LogisticRegression(),\n",
    "            cv=3,\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        return ensemble\n",
    "\n",
    "{RealExoplanetModel.train.__doc__}\n",
    "    def train(self, X, y):\n",
    "        if X is None or y is None:\n",
    "            st.error(\"‚ùå No hay datos para entrenar\")\n",
    "            return None\n",
    "\n",
    "        st.info(\"ü§ñ Iniciando entrenamiento del ensemble...\")\n",
    "        self.model = self.create_ensemble()\n",
    "        self.model.fit(X, y)\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.accuracy = accuracy_score(y, y_pred)\n",
    "        st.write(f\"üìà Accuracy en entrenamiento: {{self.accuracy:.2%}}\")\n",
    "        self._calculate_feature_importance(X.shape[1])\n",
    "        return self.model\n",
    "\n",
    "{RealExoplanetModel._calculate_feature_importance.__doc__}\n",
    "    def _calculate_feature_importance(self, n_features):\n",
    "        importances = np.zeros(n_features)\n",
    "        estimators_with_importance = 0\n",
    "        for name, model in self.model.named_estimators_.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances += model.feature_importances_\n",
    "                estimators_with_importance += 1\n",
    "\n",
    "        if estimators_with_importance > 0:\n",
    "            self.feature_importance = importances / estimators_with_importance\n",
    "        else:\n",
    "            self.feature_importance = None\n",
    "\n",
    "{RealExoplanetModel.save_model.__doc__}\n",
    "    def save_model(self, filepath):\n",
    "        if self.model:\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            joblib.dump(self.model, filepath)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "{RealExoplanetModel.load_model.__doc__}\n",
    "    def load_model(self, filepath):\n",
    "        try:\n",
    "            if os.path.exists(filepath):\n",
    "                self.model = joblib.load(filepath)\n",
    "                self.accuracy = 0\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error cargando modelo: {{e}}\")\n",
    "        return False\n",
    "\n",
    "{ExoplanetDetectorApp.__doc__}\n",
    "class ExoplanetDetectorApp:\n",
    "{ExoplanetDetectorApp.__init__.__doc__}\n",
    "    def __init__(self):\n",
    "        self.model = RealExoplanetModel()\n",
    "        self.data_processor = ExoplanetDataProcessor()\n",
    "        self.model_trained = False\n",
    "        self.load_initial_state()\n",
    "\n",
    "    def load_initial_state(self):\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "\n",
    "        if os.path.exists(model_path) and self.model.load_model(model_path):\n",
    "            self.model_trained = True\n",
    "\n",
    "        if os.path.exists(processor_path):\n",
    "            try:\n",
    "                self.data_processor = joblib.load(processor_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if os.path.exists(features_path):\n",
    "            try:\n",
    "                self.data_processor.feature_names = joblib.load(features_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "{ExoplanetDetectorApp.render_sidebar.__doc__}\n",
    "    def render_sidebar(self):\n",
    "        st.sidebar.title(\"üî≠ NASA Exoplanet Detector - REAL\")\n",
    "        st.sidebar.markdown(\"---\")\n",
    "        page = st.sidebar.radio(\"Navegaci√≥n\", [\n",
    "            \"üè† Inicio\", \"üöÄ Entrenar Modelo REAL\", \"ü§ñ Clasificar Exoplanetas\",\n",
    "            \"üì¶ Clasificaci√≥n por Lotes\", \"üìä An√°lisis de Datos REAL\", \"üíæ Modelos Guardados\"\n",
    "        ])\n",
    "        st.sidebar.markdown(\"---\")\n",
    "        st.sidebar.info(\"Sistema REAL con datos de Kepler, K2 y TESS de la NASA\")\n",
    "        return page\n",
    "\n",
    "{ExoplanetDetectorApp.render_home.__doc__}\n",
    "    def render_home(self):\n",
    "        st.title(\"ü™ê NASA Exoplanet Detection AI - SISTEMA REAL\")\n",
    "        col1, col2 = st.columns([2, 1])\n",
    "        with col1:\n",
    "            st.markdown(\"### Sistema REAL de Detecci√≥n de Exoplanetas\\\\n\\\\n**Caracter√≠sticas IMPLEMENTADAS:**\\\\n- ‚úÖ **Entrenamiento REAL** con datos de la NASA\\\\n- ‚úÖ **Modelos PERSISTENTES** que se guardan en disco\\\\n- ‚úÖ **Datos REALES** Kepler, K2 y TESS\\\\n- ‚úÖ **Ensemble Stacking** como en el paper cient√≠fico\\\\n- ‚úÖ **Guardado/Auto-carga** de modelos\\\\n\\\\n**Para comenzar:**\\\\n1. Sube tus archivos CSV a `data/raw/`\\\\n2. Ve a **'Entrenar Modelo REAL'**\\\\n3. ¬°El sistema detectar√° autom√°ticamente tus datos!\\\\n\")\n",
    "\n",
    "        with col2:\n",
    "            st.image(\"https://www.nasa.gov/sites/default/files/thumbnails/image/kepler_all_planets_art.jpg\",\n",
    "                     use_column_width=True, caption=\"Datos REALES de la NASA\")\n",
    "\n",
    "        st.subheader(\"üîç Verificaci√≥n de Archivos\")\n",
    "        data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "        if os.path.exists(data_dir):\n",
    "            files = os.listdir(data_dir)\n",
    "            csv_files = [f for f in files if f.endswith('.csv')]\n",
    "            if csv_files:\n",
    "                st.success(f\"‚úÖ Directorio data/raw/ encontrado\")\n",
    "                st.write(f\"üìÑ Archivos CSV: {{csv_files}}\")\n",
    "            else:\n",
    "                st.warning(f\"‚ö†Ô∏è Directorio existe pero no hay archivos CSV\")\n",
    "        else:\n",
    "            st.error(f\"‚ùå No existe el directorio: {{data_dir}}\")\n",
    "            st.info(\"Soluci√≥n: Crea la carpeta `data/raw/` y coloca tus archivos.\")\n",
    "\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if os.path.exists(model_path):\n",
    "            st.success(\"‚úÖ **Modelo entrenado disponible**\")\n",
    "            if self.model_trained:\n",
    "                st.metric(\"Modelo Cargado\", \"Ensemble Stacking\")\n",
    "            else:\n",
    "                 st.warning(\"Modelo existe, pero la autocarga fall√≥. Vuelve a entrenar si es necesario.\")\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è **No hay modelo entrenado** - Ve a 'Entrenar Modelo REAL' para comenzar\")\n",
    "\n",
    "{ExoplanetDetectorApp.render_real_training.__doc__}\n",
    "    def render_real_training(self):\n",
    "        st.title(\"üöÄ Entrenamiento REAL con Datos NASA\")\n",
    "        st.info(\"Entrenamiento REAL del modelo Ensemble usando tus datasets de la NASA.\")\n",
    "\n",
    "        if st.button(\"üéØ Iniciar Entrenamiento REAL\", type=\"primary\"):\n",
    "            with st.spinner(\"Cargando y procesando datos REALES de la NASA...\"):\n",
    "                try:\n",
    "                    kepler_df, k2_df, tess_df = self.data_processor.load_real_data()\n",
    "\n",
    "                    if kepler_df is None or kepler_df.empty:\n",
    "                        st.error(\"‚ùå No se pudieron cargar los datasets o Kepler est√° vac√≠o\")\n",
    "                        return\n",
    "\n",
    "                    st.subheader(\"üìä Datasets Cargados\")\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "                    with col1: st.metric(\"Kepler\", f\"{{len(kepler_df):,}} registros\")\n",
    "                    with col2: st.metric(\"K2\", f\"{{len(k2_df) if k2_df is not None else 0:,}} registros\")\n",
    "                    with col3: st.metric(\"TESS\", f\"{{len(tess_df) if tess_df is not None else 0:,}} registros\")\n",
    "\n",
    "                    st.subheader(\"üîß Procesando Datos...\")\n",
    "                    kepler_processed = self.data_processor.preprocess_kepler(kepler_df)\n",
    "                    if kepler_processed is None: return\n",
    "\n",
    "                    datasets_to_process = [kepler_processed]\n",
    "                    if k2_df is not None:\n",
    "                        k2_processed = self.data_processor.preprocess_k2(k2_df)\n",
    "                        if k2_processed is not None: datasets_to_process.append(k2_processed)\n",
    "                    if tess_df is not None:\n",
    "                        tess_processed = self.data_processor.preprocess_tess(tess_df)\n",
    "                        if tess_processed is not None: datasets_to_process.append(tess_processed)\n",
    "\n",
    "                    datasets_to_process = [d for d in datasets_to_process if d is not None and not d.empty]\n",
    "                    if not datasets_to_process:\n",
    "                        st.error(\"‚ùå No hay datos v√°lidos para procesar\")\n",
    "                        return\n",
    "\n",
    "                    unified_data = pd.concat(datasets_to_process, ignore_index=True)\n",
    "                    st.success(f\"‚úÖ Datos unificados: {{len(unified_data):,}} muestras\")\n",
    "                    X, y, feature_names = self.data_processor.prepare_features(unified_data)\n",
    "\n",
    "                    if X is None:\n",
    "                        st.error(\"‚ùå No se pudieron preparar las caracter√≠sticas\")\n",
    "                        return\n",
    "\n",
    "                    st.subheader(\"ü§ñ Entrenando Modelo Ensemble...\")\n",
    "                    trained_model = self.model.train(X, y)\n",
    "\n",
    "                    if trained_model is None:\n",
    "                        st.error(\"‚ùå Error en el entrenamiento\")\n",
    "                        return\n",
    "\n",
    "                    models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "                    model_path = os.path.join(models_dir, 'real_ensemble_model.pkl')\n",
    "                    processor_path = os.path.join(models_dir, 'data_processor.pkl')\n",
    "                    features_path = os.path.join(models_dir, 'feature_names.pkl')\n",
    "\n",
    "                    if self.model.save_model(model_path):\n",
    "                        joblib.dump(self.data_processor, processor_path)\n",
    "                        joblib.dump(feature_names, features_path)\n",
    "                        st.success(\"‚úÖ Modelo entrenado y guardado exitosamente!\")\n",
    "                        self.model_trained = True\n",
    "\n",
    "                        st.subheader(\"üìà Resultados del Entrenamiento\")\n",
    "                        col1, col2, col3, col4 = st.columns(4)\n",
    "                        with col1: st.metric(\"Accuracy\", f\"{{self.model.accuracy:.2%}}\")\n",
    "                        with col2: st.metric(\"Muestras\", f\"{{X.shape[0]:,}}\")\n",
    "                        with col3: st.metric(\"Caracter√≠sticas\", X.shape[1])\n",
    "                        with col4: st.metric(\"Algoritmos\", \"4 Ensemble\")\n",
    "\n",
    "                        if self.model.feature_importance is not None and feature_names and len(feature_names) == len(self.model.feature_importance):\n",
    "                            st.subheader(\"üîç Importancia de Caracter√≠sticas\")\n",
    "                            importance_df = pd.DataFrame({{\n",
    "                                'Caracter√≠stica': feature_names,\n",
    "                                'Importancia': self.model.feature_importance\n",
    "                            }}).sort_values('Importancia', ascending=False)\n",
    "                            fig = px.bar(importance_df.head(10), x='Importancia', y='Caracter√≠stica', title='Top 10 Caracter√≠sticas M√°s Importantes', orientation='h')\n",
    "                            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "                    st.balloons()\n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error durante el entrenamiento: {{str(e)}}\")\n",
    "                    import traceback\n",
    "                    st.code(traceback.format_exc())\n",
    "\n",
    "{ExoplanetDetectorApp.render_real_classification.__doc__}\n",
    "    def render_real_classification(self):\n",
    "        st.title(\"ü§ñ Clasificaci√≥n con Modelo REAL\")\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "\n",
    "        if not os.path.exists(model_path) or not os.path.exists(features_path) or not os.path.exists(processor_path):\n",
    "            st.warning(\"‚ö†Ô∏è **No hay modelo o archivos de preprocesamiento entrenados**\")\n",
    "            return\n",
    "\n",
    "        if not self.model_trained:\n",
    "            self.load_initial_state()\n",
    "            if not self.model_trained:\n",
    "                st.error(\"‚ùå Error cargando el modelo o el preprocesador.\")\n",
    "                return\n",
    "\n",
    "        st.success(\"‚úÖ Modelo REAL y preprocesador cargados exitosamente\")\n",
    "        feature_names = self.data_processor.feature_names\n",
    "        num_features = len(feature_names)\n",
    "        st.info(f\"üîç El modelo espera **{{num_features}}** caracter√≠sticas: {{', '.join(feature_names)}}\")\n",
    "\n",
    "        # Definiciones de inputs\n",
    "        input_definitions = [\n",
    "            (\"koi_period\", \"Per√≠odo Orbital (d√≠as)\", 10.0, 0.1, 1000.0),\n",
    "            (\"koi_duration\", \"Duraci√≥n Tr√°nsito (horas)\", 3.0, 0.1, 24.0),\n",
    "            (\"koi_depth\", \"Profundidad Tr√°nsito (ppm)\", 500, 1, 100000),\n",
    "            (\"koi_prad\", \"Radio Planetario (Radios Tierra)\", 2.0, 0.1, 50.0),\n",
    "            (\"koi_teq\", \"Temperatura Equilibrio (K)\", 500, 100, 5000),\n",
    "            (\"koi_insol\", \"Flujo de Insolaci√≥n\", 100.0, 0.1, 10000.0),\n",
    "            (\"koi_steff\", \"Temperatura Estelar (K)\", 5800, 2000, 15000),\n",
    "            (\"koi_slogg\", \"Gravedad Estelar (log g)\", 4.4, 3.0, 5.5),\n",
    "            (\"koi_srad\", \"Radio Estelar (Radios Sol)\", 1.0, 0.1, 10.0),\n",
    "        ]\n",
    "        feature_map = {{name: defs for name, *defs in input_definitions}}\n",
    "        input_values = {{}}\n",
    "\n",
    "        with st.form(\"real_classification_form\"):\n",
    "            st.subheader(f\"üìê Par√°metros del Candidato - {{num_features}} CARACTER√çSTICAS REQUERIDAS\")\n",
    "            cols = st.columns(min(2, num_features))\n",
    "            col_index = 0\n",
    "\n",
    "            for feature_name in feature_names:\n",
    "                if feature_name in feature_map:\n",
    "                    friendly_name, default_value, min_val, max_val = feature_map[feature_name]\n",
    "                    with cols[col_index % 2]:\n",
    "                        input_values[feature_name] = st.number_input(friendly_name, min_value=min_val, max_value=max_val, value=default_value, key=f\"input_{feature_name}\")\n",
    "                    col_index += 1\n",
    "                else:\n",
    "                    st.warning(f\"‚ö†Ô∏è Caracter√≠stica esperada **{{feature_name}}** no encontrada en la definici√≥n del formulario.\")\n",
    "\n",
    "            submitted = st.form_submit_button(\"üöÄ Clasificar con Modelo REAL\")\n",
    "\n",
    "        if submitted:\n",
    "            final_features = tuple(input_values[name] for name in feature_names if name in input_values)\n",
    "            if len(final_features) == num_features:\n",
    "                self._real_prediction(*final_features)\n",
    "            else:\n",
    "                 st.error(f\"‚ùå ERROR: Env√≠as {{len(final_features)}} caracter√≠sticas. Modelo espera {{num_features}}.\")\n",
    "\n",
    "{ExoplanetDetectorApp._real_prediction.__doc__}\n",
    "    def _real_prediction(self, *features):\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        features_path = os.path.join(PROJECT_ROOT, 'models', 'feature_names.pkl')\n",
    "        try:\n",
    "            saved_feature_names = joblib.load(features_path)\n",
    "            data_processor = joblib.load(processor_path)\n",
    "\n",
    "            if len(features) != len(saved_feature_names):\n",
    "                st.error(\"‚ùå ERROR CR√çTICO - Discrepancia en caracter√≠sticas.\")\n",
    "                return\n",
    "\n",
    "            feature_array = np.array([features]).reshape(1, -1)\n",
    "            feature_array_scaled = data_processor.scaler.transform(feature_array)\n",
    "            prediction = self.model.model.predict(feature_array_scaled)[0]\n",
    "            probability = self.model.model.predict_proba(feature_array_scaled)[0, 1]\n",
    "\n",
    "            st.subheader(\"üéØ Resultado de la Clasificaci√≥n REAL\")\n",
    "            col1, col2 = st.columns([1, 2])\n",
    "\n",
    "            with col1:\n",
    "                if prediction == 1:\n",
    "                    st.success(\"‚úÖ **EXOPLANETA DETECTADO**\")\n",
    "                    st.balloons()\n",
    "                else:\n",
    "                    st.error(\"‚ùå **NO ES EXOPLANETA**\")\n",
    "                st.metric(\"Probabilidad\", f\"{{probability:.2%}}\")\n",
    "                st.info(\"üü¢ Alta confianza\" if probability >= 0.8 else \"üü° Confianza media\" if probability >= 0.6 else \"üî¥ Baja confianza\")\n",
    "\n",
    "            with col2:\n",
    "                st.markdown(\"#### üìä An√°lisis de Caracter√≠sticas\")\n",
    "                feature_display_names = {{'koi_period': 'Per√≠odo Orbital', 'koi_duration': 'Duraci√≥n Tr√°nsito', 'koi_depth': 'Profundidad Tr√°nsito', 'koi_prad': 'Radio Planetario', 'koi_teq': 'Temperatura Planeta', 'koi_insol': 'Flujo Insolaci√≥n', 'koi_steff': 'Temperatura Estelar', 'koi_slogg': 'Gravedad Estelar', 'koi_srad': 'Radio Estelar'}}\n",
    "                feature_units = {{'koi_period': 'd√≠as', 'koi_duration': 'horas', 'koi_depth': 'ppm', 'koi_prad': 'R‚äï', 'koi_teq': 'K', 'koi_insol': 'S‚äï', 'koi_steff': 'K', 'koi_slogg': 'log g', 'koi_srad': 'R‚òâ'}}\n",
    "                analysis_data = []\n",
    "                for i, feature_name in enumerate(saved_feature_names):\n",
    "                    analysis_data.append({{\n",
    "                        'Caracter√≠stica': feature_display_names.get(feature_name, feature_name),\n",
    "                        'Valor': f\"{{features[i]}} {{feature_units.get(feature_name, '')}}\",\n",
    "                        'C√≥digo': feature_name\n",
    "                    }})\n",
    "                st.dataframe(pd.DataFrame(analysis_data), use_container_width=True, hide_index=True)\n",
    "                st.markdown(\"#### üí° Informaci√≥n del Modelo\")\n",
    "                st.info(f\"- **Modelo:** Ensemble Stacking (4 algoritmos)\\\\n- **Caracter√≠sticas:** {{len(saved_feature_names)}}\\\\n- **Precisi√≥n:** ~{{self.model.accuracy:.2%}}\\\\n- **Datos de entrenamiento:** Kepler + K2 + TESS (NASA)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error en la predicci√≥n: {{e}}\")\n",
    "            import traceback\n",
    "            st.code(traceback.format_exc())\n",
    "\n",
    "{ExoplanetDetectorApp.render_batch_classification.__doc__}\n",
    "    def render_batch_classification(self):\n",
    "        st.title(\"üì¶ Clasificaci√≥n por Lotes - (En Desarrollo)\")\n",
    "        st.info(\"Esta funcionalidad te permitir√° subir un archivo CSV para clasificar m√∫ltiples candidatos a exoplanetas.\")\n",
    "        st.warning(\"Deber√≠as implementar aqu√≠ la l√≥gica para subir un archivo, preprocesarlo (usando el DataProcessor cargado) y mostrar los resultados.\")\n",
    "\n",
    "{ExoplanetDetectorApp.render_data_analysis.__doc__}\n",
    "    def render_data_analysis(self):\n",
    "        st.title(\"üìä An√°lisis de Datos REAL - (En Desarrollo)\")\n",
    "        st.info(\"Esta secci√≥n podr√≠a mostrar gr√°ficos de dispersi√≥n, histogramas de las caracter√≠sticas, y la distribuci√≥n de clases (CONFIRMED vs. FALSE POSITIVE) del dataset unificado.\")\n",
    "\n",
    "{ExoplanetDetectorApp.render_saved_models.__doc__}\n",
    "    def render_saved_models(self):\n",
    "        st.title(\"üíæ Modelos Guardados\")\n",
    "        models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "        st.info(f\"Buscando archivos en: {{models_dir}}\")\n",
    "        if os.path.exists(models_dir):\n",
    "            files = os.listdir(models_dir)\n",
    "            model_files = [f for f in files if f.endswith('.pkl')]\n",
    "            if model_files:\n",
    "                st.success(\"Archivos de modelo encontrados:\")\n",
    "                st.write(model_files)\n",
    "            else:\n",
    "                st.warning(\"No se encontraron archivos de modelo (.pkl).\")\n",
    "        else:\n",
    "            st.error(\"Directorio de modelos no encontrado.\")\n",
    "\n",
    "{ExoplanetDetectorApp.run.__doc__}\n",
    "    def run(self):\n",
    "        st.set_page_config(layout=\"wide\", page_title=\"NASA Exoplanet Detector\")\n",
    "        page = self.render_sidebar()\n",
    "\n",
    "        if page == \"üè† Inicio\":\n",
    "            self.render_home()\n",
    "        elif page == \"üöÄ Entrenar Modelo REAL\":\n",
    "            self.render_real_training()\n",
    "        elif page == \"ü§ñ Clasificar Exoplanetas\":\n",
    "            self.render_real_classification()\n",
    "        elif page == \"üì¶ Clasificaci√≥n por Lotes\":\n",
    "            self.render_batch_classification()\n",
    "        elif page == \"üìä An√°lisis de Datos REAL\":\n",
    "            self.render_data_analysis()\n",
    "        elif page == \"üíæ Modelos Guardados\":\n",
    "            self.render_saved_models()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = ExoplanetDetectorApp()\n",
    "    app.run()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Escribir el c√≥digo en un archivo para que Streamlit pueda ejecutarlo\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(APP_FILE_CONTENT)\n",
    "\n",
    "# 5. EJECUCI√ìN DE STREAMLIT CON NGROK\n",
    "# ====================================\n",
    "\n",
    "# Establecer la clave de ngrok (necesitas una cuenta gratuita en ngrok.com)\n",
    "# Si no tienes, puedes probar sin ella, pero a menudo se necesita para t√∫neles persistentes.\n",
    "# !ngrok authtoken <TU_CLAVE_DE_NGROK>\n",
    "\n",
    "# Iniciar ngrok\n",
    "print(\"Iniciando ngrok...\")\n",
    "try:\n",
    "    # Usar un puerto diferente de 8501 para evitar conflictos\n",
    "    public_url = ngrok.connect(port=8501)\n",
    "    print(f\"**Tu URL de Streamlit (ngrok):** {public_url}\")\n",
    "\n",
    "    # Ejecutar Streamlit en segundo plano\n",
    "    print(\"Ejecutando Streamlit...\")\n",
    "    p = subprocess.Popen(\n",
    "        [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--browser.gatherUsageStats\", \"False\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    # Dar tiempo para que Streamlit se inicie\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Mostrar el enlace al usuario\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"**¬°ABRE ESTE ENLACE EN TU NAVEGADOR!** üöÄ\")\n",
    "    print(f\"**{public_url}**\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Mantener el proceso Streamlit corriendo.\n",
    "    # Necesitas que la celda de Colab siga activa para mantener el t√∫nel y el proceso.\n",
    "    # Bucle infinito para no finalizar el script.\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Streamlit detenido por el usuario.\")\n",
    "    finally:\n",
    "        p.terminate()\n",
    "        ngrok.kill()\n",
    "        print(\"T√∫nel ngrok y proceso Streamlit finalizados.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al iniciar ngrok o Streamlit: {e}\")\n",
    "    print(\"Aseg√∫rate de haber instalado 'pyngrok' y tener una conexi√≥n a internet estable.\")\n",
    "    print(\"Si el error persiste, usa 'streamlit run app.py --server.port 8501' y busca una alternativa a ngrok si es necesario.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
