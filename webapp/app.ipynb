{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c0a90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. INSTALACI√ìN Y CONFIGURACI√ìN INICIAL\n",
    "# ==============================================================================\n",
    "# Instalar bibliotecas necesarias\n",
    "!pip install streamlit joblib pandas numpy plotly scikit-learn lightgbm xgboost\n",
    "\n",
    "# Instalar ngrok y pyngrok para visualizar Streamlit en Colab\n",
    "!pip install pyngrok\n",
    "\n",
    "# Importar bibliotecas\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import warnings\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. GESTI√ìN DE RUTAS Y ESTRUCTURA DE CARPETAS PARA COLAB\n",
    "# ==============================================================================\n",
    "# Definir la ruta ra√≠z del proyecto para Colab\n",
    "def get_project_root():\n",
    "    \"\"\"Obtener la ruta ra√≠z del proyecto: el directorio actual en Colab.\"\"\"\n",
    "    return os.getcwd()\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "\n",
    "# Crear la estructura de directorios necesaria\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'data', 'raw'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROJECT_ROOT, 'models'), exist_ok=True)\n",
    "\n",
    "print(f\"Carpeta de datos creada en: {os.path.join(PROJECT_ROOT, 'data', 'raw')}\")\n",
    "print(\"¬°Recuerda subir tus CSV de la NASA a esta carpeta!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DEFINICI√ìN DE CLASES Y FUNCIONES (Backend Puro)\n",
    "# ==============================================================================\n",
    "\n",
    "class ExoplanetDataProcessor:\n",
    "    \"\"\"\n",
    "    Procesador de datos para los datasets reales de la NASA.\n",
    "    (Backend puro, NO usa st.info, st.error, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "\n",
    "    def load_real_data(self):\n",
    "        \"\"\"Cargar los datasets reales de la NASA con rutas corregidas\"\"\"\n",
    "        try:\n",
    "            data_dir = os.path.join(PROJECT_ROOT, 'data', 'raw')\n",
    "\n",
    "            print(f\"üîç Buscando datos en: {data_dir}\")\n",
    "\n",
    "            if not os.path.exists(data_dir):\n",
    "                print(f\"‚ùå No existe el directorio: {data_dir}\")\n",
    "                return None, None, None\n",
    "\n",
    "            files = os.listdir(data_dir)\n",
    "            print(f\"üìÅ Archivos encontrados en data/raw/: {files}\")\n",
    "\n",
    "            # Construir rutas completas\n",
    "            kepler_path = os.path.join(data_dir, 'kepler.csv')\n",
    "            k2_path = os.path.join(data_dir, 'k2.csv')\n",
    "            tess_path = os.path.join(data_dir, 'tess.csv')\n",
    "\n",
    "            print(f\"üìä Intentando cargar:\\n- {kepler_path}\\n- {k2_path}\\n- {tess_path}\")\n",
    "\n",
    "            if not os.path.exists(kepler_path):\n",
    "                print(f\"‚ùå No existe: {kepler_path}. Por favor, s√∫belo.\")\n",
    "                return None, None, None\n",
    "\n",
    "            # Cargar los archivos reales\n",
    "            kepler_df = pd.read_csv(kepler_path)\n",
    "            k2_df = pd.read_csv(k2_path) if os.path.exists(k2_path) else None\n",
    "            tess_df = pd.read_csv(tess_path) if os.path.exists(tess_path) else None\n",
    "\n",
    "            print(f\"‚úÖ Kepler cargado: {len(kepler_df)} registros\")\n",
    "            if k2_df is not None:\n",
    "                print(f\"‚úÖ K2 cargado: {len(k2_df)} registros\")\n",
    "            if tess_df is not None:\n",
    "                print(f\"‚úÖ TESS cargado: {len(tess_df)} registros\")\n",
    "\n",
    "            return kepler_df, k2_df, tess_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando datasets: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def preprocess_kepler(self, df):\n",
    "        \"\"\"Preprocesar datos Kepler reales\"\"\"\n",
    "        df_clean = df.copy()\n",
    "\n",
    "        if 'koi_disposition' not in df_clean.columns:\n",
    "            print(\"‚ùå No se encuentra la columna 'koi_disposition' en Kepler\")\n",
    "            return None\n",
    "\n",
    "        # Eliminar columnas no √∫tiles\n",
    "        columns_to_drop = ['kepid', 'kepoi_name', 'kepler_name', 'koi_pdisposition', 'koi_score']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df_clean.columns]\n",
    "        if columns_to_drop:\n",
    "            df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "\n",
    "        # Filtrar solo confirmed, candidate y false positive\n",
    "        valid_dispositions = ['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']\n",
    "        mask = df_clean['koi_disposition'].isin(valid_dispositions)\n",
    "        df_clean = df_clean[mask]\n",
    "\n",
    "        # Crear target binario\n",
    "        df_clean['target'] = df_clean['koi_disposition'].map({\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1,\n",
    "            'FALSE POSITIVE': 0\n",
    "        })\n",
    "\n",
    "        # A√±adir identificador de misi√≥n\n",
    "        df_clean['mission'] = 'kepler'\n",
    "\n",
    "        print(f\"‚úÖ Kepler procesado: {len(df_clean)} registros v√°lidos\")\n",
    "\n",
    "        return df_clean\n",
    "\n",
    "    def preprocess_k2(self, df):\n",
    "        \"\"\"Preprocesar datos K2 reales\"\"\"\n",
    "        if df is None: return None\n",
    "\n",
    "        df_clean = df.copy()\n",
    "        if 'disposition' not in df_clean.columns:\n",
    "            print(\"‚ùå No se encuentra la columna 'disposition' en K2\")\n",
    "            return None\n",
    "\n",
    "        df_clean = df_clean[df_clean['disposition'].isin(['CONFIRMED', 'CANDIDATE'])]\n",
    "\n",
    "        # Target binario\n",
    "        df_clean['target'] = df_clean['disposition'].map({\n",
    "            'CONFIRMED': 1,\n",
    "            'CANDIDATE': 1\n",
    "        })\n",
    "        df_clean['mission'] = 'k2'\n",
    "        print(f\"‚úÖ K2 procesado: {len(df_clean)} registros v√°lidos\")\n",
    "        return df_clean\n",
    "\n",
    "    def preprocess_tess(self, df):\n",
    "        \"\"\"Preprocesar datos TESS reales\"\"\"\n",
    "        if df is None: return None\n",
    "\n",
    "        df_clean = df.copy()\n",
    "        if 'tfopwg_disp' not in df_clean.columns:\n",
    "            print(\"‚ùå No se encuentra la columna 'tfopwg_disp' en TESS\")\n",
    "            return None\n",
    "\n",
    "        # Mapear disposiciones de TESS\n",
    "        disposition_mapping = {\n",
    "            'PC': 1, 'KP': 1, 'APC': 1,  # Positivos\n",
    "            'FP': 0, 'FA': 0  # Negativos\n",
    "        }\n",
    "\n",
    "        df_clean['target'] = df_clean['tfopwg_disp'].map(disposition_mapping)\n",
    "        df_clean = df_clean.dropna(subset=['target'])\n",
    "        df_clean['mission'] = 'tess'\n",
    "        print(f\"‚úÖ TESS procesado: {len(df_clean)} registros v√°lidos\")\n",
    "        return df_clean\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"Preparar caracter√≠sticas para el modelo - VERSI√ìN FLEXIBLE\"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            print(\"‚ùå No hay datos para preparar caracter√≠sticas\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Posibles nombres de columnas en diferentes datasets\n",
    "        possible_features = {\n",
    "            'orbital_period': ['koi_period', 'pl_orbper', 'period'],\n",
    "            'transit_duration': ['koi_duration', 'pl_trandurh', 'duration'],\n",
    "            'transit_depth': ['koi_depth', 'pl_trandep', 'depth'],\n",
    "            'planet_radius': ['koi_prad', 'pl_rade', 'radius'],\n",
    "            'equilibrium_temp': ['koi_teq', 'pl_eqt', 'teq'],\n",
    "            'insolation_flux': ['koi_insol', 'pl_insol', 'insol'],\n",
    "            'stellar_teff': ['koi_steff', 'st_teff', 'teff'],\n",
    "            'stellar_logg': ['koi_slogg', 'st_logg', 'logg'],\n",
    "            'stellar_radius': ['koi_srad', 'st_rad', 'srad']\n",
    "        }\n",
    "\n",
    "        # Encontrar las columnas disponibles\n",
    "        available_columns = []\n",
    "        for feature_name, possible_names in possible_features.items():\n",
    "            for name in possible_names:\n",
    "                if name in df.columns:\n",
    "                    available_columns.append(name)\n",
    "                    break\n",
    "\n",
    "        print(f\"üìä Columnas num√©ricas encontradas: {available_columns}\")\n",
    "\n",
    "        if not available_columns:\n",
    "            print(\"‚ùå No se encontraron columnas num√©ricas para entrenar\")\n",
    "            return None, None, None\n",
    "\n",
    "        X = df[available_columns].copy()\n",
    "        y = df['target'].values\n",
    "\n",
    "        # Manejar valores missing\n",
    "        X = X.fillna(X.median())\n",
    "\n",
    "        # Escalar caracter√≠sticas: fit_transform para entrenamiento\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.feature_names = available_columns # Guardar los nombres de las features\n",
    "        print(f\"‚úÖ Caracter√≠sticas preparadas: {X_scaled.shape}\")\n",
    "\n",
    "        return X_scaled, y, available_columns\n",
    "\n",
    "class RealExoplanetModel:\n",
    "    \"\"\"\n",
    "    Modelo real para entrenamiento con datos de la NASA.\n",
    "    (Backend puro, NO usa st.info, st.error, etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.accuracy = 0\n",
    "        self.feature_importance = None\n",
    "\n",
    "    def create_ensemble(self):\n",
    "        \"\"\"Crear ensemble con los algoritmos del paper\"\"\"\n",
    "        base_models = [\n",
    "            ('random_forest', RandomForestClassifier(\n",
    "                n_estimators=100, max_depth=10, min_samples_split=5, random_state=42, n_jobs=-1\n",
    "            )),\n",
    "            ('extra_trees', ExtraTreesClassifier(\n",
    "                n_estimators=100, max_depth=10, random_state=42, n_jobs=-1\n",
    "            )),\n",
    "            ('xgboost', XGBClassifier(\n",
    "                n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, use_label_encoder=False, eval_metric='logloss'\n",
    "            )),\n",
    "            ('lightgbm', LGBMClassifier(\n",
    "                n_estimators=100, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1, verbose=-1 # Silenciar LightGBM\n",
    "            ))\n",
    "        ]\n",
    "\n",
    "        ensemble = StackingClassifier(\n",
    "            estimators=base_models,\n",
    "            final_estimator=LogisticRegression(),\n",
    "            cv=3,\n",
    "            passthrough=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        return ensemble\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Entrenar el modelo real\"\"\"\n",
    "        if X is None or y is None:\n",
    "            print(\"‚ùå No hay datos para entrenar\")\n",
    "            return None\n",
    "\n",
    "        print(\"ü§ñ Iniciando entrenamiento del ensemble...\")\n",
    "\n",
    "        self.model = self.create_ensemble()\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        # Calcular accuracy en entrenamiento\n",
    "        y_pred = self.model.predict(X)\n",
    "        self.accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "        print(f\"üìà Accuracy en entrenamiento: {self.accuracy:.2%}\")\n",
    "\n",
    "        # Calcular importancia de caracter√≠sticas\n",
    "        self._calculate_feature_importance(X.shape[1])\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def _calculate_feature_importance(self, n_features):\n",
    "        \"\"\"Calcular importancia de caracter√≠sticas promediada\"\"\"\n",
    "        importances = np.zeros(n_features)\n",
    "        count = 0\n",
    "\n",
    "        for name, model in self.model.named_estimators_.items():\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances += model.feature_importances_\n",
    "                count += 1\n",
    "\n",
    "        if count > 0:\n",
    "            self.feature_importance = importances / count\n",
    "        else:\n",
    "            self.feature_importance = None\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Guardar modelo entrenado\"\"\"\n",
    "        if self.model:\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            joblib.dump(self.model, filepath)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Cargar modelo entrenado\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(filepath):\n",
    "                self.model = joblib.load(filepath)\n",
    "                self.accuracy = 0\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CLASE PRINCIPAL DE STREAMLIT (Frontend)\n",
    "# ==============================================================================\n",
    "\n",
    "class ExoplanetDetectorApp:\n",
    "    def __init__(self):\n",
    "        self.model = RealExoplanetModel()\n",
    "        self.data_processor = ExoplanetDataProcessor()\n",
    "        self.model_trained = False\n",
    "\n",
    "        self.load_initial_state()\n",
    "\n",
    "    def load_initial_state(self):\n",
    "        \"\"\"Cargar modelo y preprocesador guardados si existen\"\"\"\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        processor_path = os.path.join(PROJECT_ROOT, 'models', 'data_processor.pkl')\n",
    "        \n",
    "        # Carga del modelo\n",
    "        if os.path.exists(model_path):\n",
    "            if self.model.load_model(model_path):\n",
    "                self.model_trained = True\n",
    "\n",
    "        # Carga del procesador (que contiene scaler y feature_names)\n",
    "        if os.path.exists(processor_path):\n",
    "            try:\n",
    "                # El procesador cargado YA contiene el .scaler y .feature_names\n",
    "                self.data_processor = joblib.load(processor_path)\n",
    "            except Exception as e:\n",
    "                pass # Si falla la carga, usa el procesador reci√©n inicializado\n",
    "\n",
    "    def render_sidebar(self):\n",
    "        \"\"\"Barra lateral de navegaci√≥n\"\"\"\n",
    "        st.sidebar.title(\"üî≠ NASA Exoplanet Detector - REAL\")\n",
    "        st.sidebar.markdown(\"---\")\n",
    "\n",
    "        page = st.sidebar.radio(\"Navegaci√≥n\", [\n",
    "            \"üè† Inicio\",\n",
    "            \"üöÄ Entrenar Modelo REAL\",\n",
    "            \"ü§ñ Clasificar Exoplanetas\",\n",
    "            \"üìä An√°lisis de Datos REAL\",\n",
    "            \"üíæ Modelos Guardados\"\n",
    "        ])\n",
    "\n",
    "        st.sidebar.markdown(\"---\")\n",
    "        st.sidebar.info(\n",
    "            \"Sistema REAL con datos de Kepler, K2 y TESS de la NASA\"\n",
    "        )\n",
    "\n",
    "        return page\n",
    "\n",
    "    def render_home(self):\n",
    "        \"\"\"P√°gina de inicio\"\"\"\n",
    "        st.title(\"ü™ê NASA Exoplanet Detection AI - SISTEMA REAL\")\n",
    "\n",
    "        col1, col2 = st.columns([2, 1])\n",
    "\n",
    "        with col1:\n",
    "            st.markdown(\"\"\"\n",
    "            ### Sistema REAL de Detecci√≥n de Exoplanetas\n",
    "            - ‚úÖ **Entrenamiento REAL** con datos de la NASA\n",
    "            - ‚úÖ **Ensemble Stacking** (RF, ET, XGB, LGBM)\n",
    "            - ‚úÖ **Guardado/Auto-carga** de modelos persistentes\n",
    "            \"\"\")\n",
    "\n",
    "            st.warning(\"\"\"\n",
    "            **ANTES DE CONTINUAR EN COLAB:**\n",
    "            1. **Sube tus CSV** (`kepler.csv`, `k2.csv`, `tess.csv`)\n",
    "               a la carpeta `data/raw/` en el explorador de archivos de Colab.\n",
    "            2. Luego, ve a **'üöÄ Entrenar Modelo REAL'**\n",
    "            \"\"\")\n",
    "        \n",
    "        with col2:\n",
    "            st.image(\"https://www.nasa.gov/sites/default/files/thumbnails/image/kepler_all_planets_art.jpg\",\n",
    "                     caption=\"Datos REALES de la NASA\")\n",
    "\n",
    "        # Verificar si hay modelo entrenado\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        if os.path.exists(model_path):\n",
    "            st.success(\"‚úÖ **Modelo entrenado disponible**\")\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è **No hay modelo entrenado** - Ve a 'Entrenar Modelo REAL'\")\n",
    "\n",
    "    def render_real_training(self):\n",
    "        \"\"\"P√°gina de entrenamiento REAL con datos de la NASA\"\"\"\n",
    "        st.title(\"üöÄ Entrenamiento REAL con Datos NASA\")\n",
    "\n",
    "        st.info(\"\"\"\n",
    "        El modelo entrenado se guardar√° autom√°ticamente y estar√° disponible para clasificaci√≥n.\n",
    "        \"\"\")\n",
    "\n",
    "        if st.button(\"üéØ Iniciar Entrenamiento REAL\", type=\"primary\"):\n",
    "            # Usar st.empty() para mostrar logs del backend si es necesario\n",
    "            log_placeholder = st.empty() \n",
    "            \n",
    "            with st.spinner(\"Cargando y procesando datos REALES de la NASA...\"):\n",
    "                try:\n",
    "                    # Cargar datos reales\n",
    "                    kepler_df, k2_df, tess_df = self.data_processor.load_real_data()\n",
    "\n",
    "                    if kepler_df is None or kepler_df.empty:\n",
    "                        st.error(\"‚ùå No se pudieron cargar los datasets o Kepler est√° vac√≠o. Verifica que los CSV est√©n en `data/raw/`.\")\n",
    "                        return\n",
    "\n",
    "                    # Procesar datos\n",
    "                    kepler_processed = self.data_processor.preprocess_kepler(kepler_df)\n",
    "                    datasets_to_process = [kepler_processed]\n",
    "                    if k2_df is not None:\n",
    "                        k2_processed = self.data_processor.preprocess_k2(k2_df)\n",
    "                        if k2_processed is not None: datasets_to_process.append(k2_processed)\n",
    "                    if tess_df is not None:\n",
    "                        tess_processed = self.data_processor.preprocess_tess(tess_df)\n",
    "                        if tess_processed is not None: datasets_to_process.append(tess_processed)\n",
    "\n",
    "                    datasets_to_process = [d for d in datasets_to_process if d is not None and not d.empty]\n",
    "                    if not datasets_to_process:\n",
    "                        st.error(\"‚ùå No hay datos v√°lidos para procesar\")\n",
    "                        return\n",
    "\n",
    "                    unified_data = pd.concat(datasets_to_process, ignore_index=True)\n",
    "                    st.success(f\"‚úÖ Datos unificados: {len(unified_data):,} muestras\")\n",
    "\n",
    "                    # Preparar caracter√≠sticas\n",
    "                    X, y, feature_names = self.data_processor.prepare_features(unified_data)\n",
    "\n",
    "                    if X is None:\n",
    "                        st.error(\"‚ùå No se pudieron preparar las caracter√≠sticas\")\n",
    "                        return\n",
    "\n",
    "                    # Entrenar modelo\n",
    "                    st.subheader(\"ü§ñ Entrenando Modelo Ensemble...\")\n",
    "                    trained_model = self.model.train(X, y)\n",
    "\n",
    "                    if trained_model is None:\n",
    "                        st.error(\"‚ùå Error en el entrenamiento\")\n",
    "                        return\n",
    "\n",
    "                    # Guardar modelo\n",
    "                    models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "                    model_path = os.path.join(models_dir, 'real_ensemble_model.pkl')\n",
    "                    processor_path = os.path.join(models_dir, 'data_processor.pkl')\n",
    "\n",
    "                    model_saved = self.model.save_model(model_path)\n",
    "\n",
    "                    if model_saved:\n",
    "                        joblib.dump(self.data_processor, processor_path)\n",
    "                        st.success(\"‚úÖ Modelo entrenado y guardado exitosamente!\")\n",
    "                        self.model_trained = True\n",
    "\n",
    "                        # Mostrar resultados\n",
    "                        st.subheader(\"üìà Resultados del Entrenamiento\")\n",
    "                        col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "                        with col1:\n",
    "                            st.metric(\"Accuracy\", f\"{self.model.accuracy:.2%}\")\n",
    "                        with col2:\n",
    "                            st.metric(\"Muestras\", f\"{X.shape[0]:,}\")\n",
    "                        with col3:\n",
    "                            st.metric(\"Caracter√≠sticas\", X.shape[1])\n",
    "                        with col4:\n",
    "                            st.metric(\"Algoritmos\", \"4 Ensemble\")\n",
    "\n",
    "                        # Importancia de caracter√≠sticas\n",
    "                        if self.model.feature_importance is not None and feature_names:\n",
    "                            st.subheader(\"üîç Importancia de Caracter√≠sticas\")\n",
    "                            if len(feature_names) == len(self.model.feature_importance):\n",
    "                                importance_df = pd.DataFrame({\n",
    "                                    'Caracter√≠stica': feature_names,\n",
    "                                    'Importancia': self.model.feature_importance\n",
    "                                }).sort_values('Importancia', ascending=False)\n",
    "\n",
    "                                fig = px.bar(\n",
    "                                    importance_df.head(10),\n",
    "                                    x='Importancia', y='Caracter√≠stica',\n",
    "                                    title='Top 10 Caracter√≠sticas M√°s Importantes',\n",
    "                                    orientation='h'\n",
    "                                )\n",
    "                                st.plotly_chart(fig, use_container_width=True)\n",
    "                            else:\n",
    "                                st.warning(\"No se pudo graficar la importancia: longitudes no coinciden.\")\n",
    "                    \n",
    "                    st.balloons()\n",
    "\n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error durante el entrenamiento: {str(e)}\")\n",
    "                    import traceback\n",
    "                    st.code(traceback.format_exc())\n",
    "\n",
    "    def render_real_classification(self):\n",
    "        \"\"\"Clasificaci√≥n con modelo REAL entrenado\"\"\"\n",
    "        st.title(\"ü§ñ Clasificaci√≥n con Modelo REAL\")\n",
    "\n",
    "        # Rutas de archivos\n",
    "        model_path = os.path.join(PROJECT_ROOT, 'models', 'real_ensemble_model.pkl')\n",
    "        \n",
    "        if not os.path.exists(model_path) or not self.model_trained:\n",
    "            st.warning(\"‚ö†Ô∏è **No hay modelo entrenado** - Ve a 'Entrenar Modelo REAL' para usar el clasificador.\")\n",
    "            return\n",
    "\n",
    "        st.success(\"‚úÖ Modelo REAL y preprocesador cargados exitosamente\")\n",
    "\n",
    "        # Cargar feature names para mostrar en la interfaz\n",
    "        feature_names = self.data_processor.feature_names\n",
    "        num_features = len(feature_names)\n",
    "        st.info(f\"üîç El modelo espera **{num_features}** caracter√≠sticas: {', '.join(feature_names)}\")\n",
    "\n",
    "        # Definir los nombres amigables y valores por defecto usando un mapa\n",
    "        feature_map_friendly = {\n",
    "            'koi_period': (\"Per√≠odo Orbital (d√≠as)\", 10.0, 0.1, 1000.0),\n",
    "            'pl_orbper': (\"Per√≠odo Orbital (d√≠as)\", 10.0, 0.1, 1000.0),\n",
    "            'period': (\"Per√≠odo Orbital (d√≠as)\", 10.0, 0.1, 1000.0),\n",
    "            'koi_duration': (\"Duraci√≥n Tr√°nsito (horas)\", 3.0, 0.1, 24.0),\n",
    "            'koi_depth': (\"Profundidad Tr√°nsito (ppm)\", 500, 1, 100000),\n",
    "            'koi_prad': (\"Radio Planetario (Radios Tierra)\", 2.0, 0.1, 50.0),\n",
    "            'koi_teq': (\"Temperatura Equilibrio (K)\", 500, 100, 5000),\n",
    "            'koi_insol': (\"Flujo de Insolaci√≥n\", 100.0, 0.1, 10000.0),\n",
    "            'koi_steff': (\"Temperatura Estelar (K)\", 5800, 2000, 15000),\n",
    "            'koi_slogg': (\"Gravedad Estelar (log g)\", 4.4, 3.0, 5.5),\n",
    "            'koi_srad': (\"Radio Estelar (Radios Sol)\", 1.0, 0.1, 10.0)\n",
    "        }\n",
    "        \n",
    "        input_values = {}\n",
    "\n",
    "        with st.form(\"real_classification_form\"):\n",
    "            st.subheader(f\"üìê Par√°metros del Candidato - {num_features} REQUERIDOS\")\n",
    "            \n",
    "            # Crear columnas din√°micas (m√°ximo 2 columnas)\n",
    "            cols = st.columns(min(2, num_features))\n",
    "            col_index = 0\n",
    "\n",
    "            # Solo mostrar los inputs para las caracter√≠sticas que el modelo REAL espera\n",
    "            for feature_name in feature_names:\n",
    "                # Buscar la definici√≥n de input, si no se encuentra, usar una predeterminada para evitar fallos\n",
    "                friendly_name, default_value, min_val, max_val = feature_map_friendly.get(\n",
    "                    feature_name, (feature_name.capitalize().replace('_', ' '), 1.0, 0.0, 100.0)\n",
    "                )\n",
    "\n",
    "                with cols[col_index % 2]:\n",
    "                    # Usar st.text_input si es string, pero para estos datos num√©ricos, number_input es mejor\n",
    "                    input_values[feature_name] = st.number_input(\n",
    "                        friendly_name,\n",
    "                        min_value=float(min_val),\n",
    "                        max_value=float(max_val),\n",
    "                        value=float(default_value),\n",
    "                        key=f\"input_{feature_name}\"\n",
    "                    )\n",
    "                col_index += 1\n",
    "\n",
    "            submitted = st.form_submit_button(\"üöÄ Clasificar con Modelo REAL\")\n",
    "\n",
    "        if submitted:\n",
    "            # 1. Asegurar el ORDEN de las caracter√≠sticas\n",
    "            final_features = tuple(input_values[name] for name in feature_names if name in input_values)\n",
    "\n",
    "            # 2. Verificar la cantidad\n",
    "            if len(final_features) == num_features:\n",
    "                self._real_prediction(*final_features)\n",
    "            else:\n",
    "                 st.error(f\"\"\"\n",
    "                 ‚ùå **ERROR DE CARACTER√çSTICAS**\n",
    "                 **Env√≠as:** {len(final_features)} | **Modelo espera:** {num_features}\n",
    "                 \"\"\")\n",
    "\n",
    "\n",
    "    def _real_prediction(self, *features):\n",
    "        \"\"\"Predicci√≥n REAL con el modelo entrenado - BLOQUE DE RESULTADOS FALTANTE CORREGIDO\"\"\"\n",
    "        try:\n",
    "            # Reutilizar el modelo y el procesador ya cargados en self\n",
    "            saved_feature_names = self.data_processor.feature_names\n",
    "            data_processor = self.data_processor\n",
    "\n",
    "            # VERIFICACI√ìN CR√çTICA: ¬øCoincide el n√∫mero de caracter√≠sticas?\n",
    "            if len(features) != len(saved_feature_names):\n",
    "                st.error(f\"\"\"\n",
    "                ‚ùå **ERROR CR√çTICO - Discrepancia en caracter√≠sticas**\n",
    "                **Env√≠as:** {len(features)} | **Modelo espera:** {len(saved_feature_names)}\n",
    "                \"\"\")\n",
    "                return\n",
    "\n",
    "            # Crear array de caracter√≠sticas\n",
    "            feature_array = np.array([features]).reshape(1, -1)\n",
    "\n",
    "            # Escalar caracter√≠sticas (usando el escalador entrenado)\n",
    "            feature_array_scaled = data_processor.scaler.transform(feature_array)\n",
    "\n",
    "            # Realizar predicci√≥n\n",
    "            prediction = self.model.model.predict(feature_array_scaled)[0]\n",
    "            probability = self.model.model.predict_proba(feature_array_scaled)[0, 1]\n",
    "\n",
    "            # ----------------------------------------------------\n",
    "            # ‚úÖ BLOQUE DE RESULTADOS A√ëADIDO (Error #3 corregido)\n",
    "            # ----------------------------------------------------\n",
    "            st.subheader(\"Resultado de la Clasificaci√≥n üßë‚Äçüî¨\")\n",
    "\n",
    "            if prediction == 1:\n",
    "                st.success(\"‚úÖ **¬°El candidato es CLASIFICADO como EXOPLANETA!**\")\n",
    "                st.markdown(f\"**Probabilidad de Exoplaneta:** `{probability:.2%}`\")\n",
    "                st.balloons()\n",
    "            else:\n",
    "                st.error(\"‚ùå **El candidato es CLASIFICADO como FALSO POSITIVO/No Exoplaneta.**\")\n",
    "                st.markdown(f\"**Probabilidad de Exoplaneta:** `{probability:.2%}`\")\n",
    "                st.markdown(\"Recomendaci√≥n: Necesita m√°s observaci√≥n o su se√±al es probable que sea ruido.\")\n",
    "            # ----------------------------------------------------\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"‚ùå Error al realizar la predicci√≥n: {e}\")\n",
    "            import traceback\n",
    "            st.code(traceback.format_exc())\n",
    "\n",
    "    def render_data_analysis(self):\n",
    "        st.title(\"üìä An√°lisis de Datos REAL\")\n",
    "        st.info(\"Esta secci√≥n es para futuras visualizaciones de los datos y el modelo.\")\n",
    "        \n",
    "    def render_models_saved(self):\n",
    "        st.title(\"üíæ Modelos Guardados\")\n",
    "        models_dir = os.path.join(PROJECT_ROOT, 'models')\n",
    "        model_path = os.path.join(models_dir, 'real_ensemble_model.pkl')\n",
    "        processor_path = os.path.join(models_dir, 'data_processor.pkl')\n",
    "\n",
    "        st.subheader(\"Archivos de Persistencia\")\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            st.success(f\"‚úÖ Modelo (real_ensemble_model.pkl) encontrado: {os.path.getsize(model_path) / (1024*1024):.2f} MB\")\n",
    "        else:\n",
    "            st.warning(\"‚ùå Modelo no encontrado. Entrena el modelo primero.\")\n",
    "        \n",
    "        if os.path.exists(processor_path):\n",
    "            st.success(f\"‚úÖ Preprocesador (data_processor.pkl) encontrado: {os.path.getsize(processor_path) / 1024:.2f} KB\")\n",
    "            st.markdown(f\"**Caracter√≠sticas esperadas:** {self.data_processor.feature_names}\")\n",
    "        else:\n",
    "            st.warning(\"‚ùå Preprocesador no encontrado. Entrena el modelo primero.\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Funci√≥n principal de Streamlit\"\"\"\n",
    "        page = self.render_sidebar()\n",
    "\n",
    "        if page == \"üè† Inicio\":\n",
    "            self.render_home()\n",
    "        elif page == \"üöÄ Entrenar Modelo REAL\":\n",
    "            self.render_real_training()\n",
    "        elif page == \"ü§ñ Clasificar Exoplanetas\":\n",
    "            self.render_real_classification()\n",
    "        elif page == \"üìä An√°lisis de Datos REAL\":\n",
    "            self.render_data_analysis()\n",
    "        elif page == \"üíæ Modelos Guardados\":\n",
    "            self.render_models_saved()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. EJECUCI√ìN DEL SCRIPT Y NGROK (Espec√≠fico para Colab)\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Crear el archivo app.py con el contenido del script de Streamlit\n",
    "    script_content = \"\"\"\n",
    "import streamlit as st\n",
    "from exoplanet_detector import ExoplanetDetectorApp\n",
    "\n",
    "def main():\n",
    "    app = ExoplanetDetectorApp()\n",
    "    app.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "    # Guardar el contenido de las clases en un m√≥dulo para importarlo\n",
    "    with open('exoplanet_detector.py', 'w') as f:\n",
    "        # Aqu√≠ se incluye todo el c√≥digo de las clases (DataProcessor, Model, App) para encapsularlo\n",
    "        f.write(f\"\"\"\n",
    "{open(__file__).read().split('class ExoplanetDetectorApp:', 1)[0]}\n",
    "class ExoplanetDetectorApp:{open(__file__).read().split('class ExoplanetDetectorApp:', 1)[1].split('# ==============================================================================\\n# 5. EJECUCI√ìN DEL SCRIPT Y NGROK', 1)[0]}\n",
    "\"\"\")\n",
    "\n",
    "    # Guardar el script de ejecuci√≥n principal de Streamlit\n",
    "    with open('app.py', 'w') as f:\n",
    "        f.write(script_content)\n",
    "\n",
    "    # 2. Ejecutar Streamlit en segundo plano\n",
    "    print(\"üöÄ Iniciando Streamlit en segundo plano...\")\n",
    "    subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
    "\n",
    "    # 3. Configurar ngrok\n",
    "    print(\"üåê Iniciando ngrok...\")\n",
    "    # Aseg√∫rate de tener tu token de ngrok si la versi√≥n m√°s reciente lo requiere\n",
    "    # ngrok.set_auth_token(\"TU_NGROK_TOKEN_AQUI\") \n",
    "    \n",
    "    # Esperar un momento para que Streamlit se inicie completamente (puerto 8501 por defecto)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Intentar establecer el t√∫nel\n",
    "    try:\n",
    "        public_url = ngrok.connect(8501)\n",
    "        print(f\"‚ú® ¬°STREAMLIT EJECUT√ÅNDOSE EN COLAB! ‚ú®\")\n",
    "        print(f\"üîó URL P√∫blica (clic aqu√≠): {public_url}\")\n",
    "        st.code(str(public_url)) # Mostrar la URL en la salida del notebook\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al conectar con ngrok: {e}\")\n",
    "        print(\"Aseg√∫rate de que streamlit est√© funcionando en el puerto 8501 y que ngrok est√© configurado.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
